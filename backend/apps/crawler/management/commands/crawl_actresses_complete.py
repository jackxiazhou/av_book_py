"""
Djangoç®¡ç†å‘½ä»¤ - è¿è¡Œå®Œæ•´å¥³å‹çˆ¬è™«
"""

import os
import sys
import subprocess
import time
from django.core.management.base import BaseCommand, CommandError
from django.conf import settings
from apps.crawler.models import CrawlTask
from datetime import datetime


class Command(BaseCommand):
    help = 'è¿è¡ŒAVMooå®Œæ•´å¥³å‹çˆ¬è™« - çˆ¬å–å¥³å‹åˆ—è¡¨ã€è¯¦æƒ…ã€ä½œå“å…³è”'

    def add_arguments(self, parser):
        parser.add_argument(
            '--max-pages',
            type=int,
            default=5,
            help='æœ€å¤§çˆ¬å–é¡µæ•° (é»˜è®¤: 5)'
        )
        parser.add_argument(
            '--max-actresses',
            type=int,
            default=50,
            help='æœ€å¤§å¥³å‹æ•°é‡ (é»˜è®¤: 50)'
        )
        parser.add_argument(
            '--output',
            type=str,
            default='actresses_complete.json',
            help='è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: actresses_complete.json)'
        )
        parser.add_argument(
            '--delay',
            type=int,
            default=3,
            help='è¯·æ±‚å»¶æ—¶ç§’æ•° (é»˜è®¤: 3)'
        )
        parser.add_argument(
            '--concurrent',
            type=int,
            default=2,
            help='å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤: 2)'
        )
        parser.add_argument(
            '--dry-run',
            action='store_true',
            help='è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…æ‰§è¡Œçˆ¬è™«'
        )
        parser.add_argument(
            '--verbose',
            action='store_true',
            help='è¯¦ç»†è¾“å‡ºæ¨¡å¼'
        )

    def handle(self, *args, **options):
        max_pages = options['max_pages']
        max_actresses = options['max_actresses']
        output_file = options['output']
        delay = options['delay']
        concurrent = options['concurrent']
        dry_run = options['dry_run']
        verbose = options['verbose']

        self.stdout.write(
            self.style.SUCCESS('ğŸ•·ï¸ å¯åŠ¨AVMooå®Œæ•´å¥³å‹çˆ¬è™«')
        )
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        self.stdout.write(f"ğŸ“‹ çˆ¬è™«é…ç½®:")
        self.stdout.write(f"   â€¢ æœ€å¤§é¡µæ•°: {max_pages}")
        self.stdout.write(f"   â€¢ æœ€å¤§å¥³å‹æ•°: {max_actresses}")
        self.stdout.write(f"   â€¢ è¾“å‡ºæ–‡ä»¶: {output_file}")
        self.stdout.write(f"   â€¢ è¯·æ±‚å»¶æ—¶: {delay}ç§’")
        self.stdout.write(f"   â€¢ å¹¶å‘æ•°: {concurrent}")
        self.stdout.write(f"   â€¢ è¯•è¿è¡Œ: {'æ˜¯' if dry_run else 'å¦'}")

        if dry_run:
            self.stdout.write(
                self.style.WARNING('ğŸ” è¯•è¿è¡Œæ¨¡å¼ - ä¸ä¼šå®é™…æ‰§è¡Œçˆ¬è™«')
            )
            return

        # åˆ›å»ºçˆ¬è™«ä»»åŠ¡è®°å½•
        task = CrawlTask.objects.create(
            spider_name='avmoo_actresses_complete',
            status='running',
            start_time=datetime.now(),
            config={
                'max_pages': max_pages,
                'max_actresses': max_actresses,
                'delay': delay,
                'concurrent': concurrent,
                'output_file': output_file
            }
        )

        try:
            # æ„å»ºçˆ¬è™«å‘½ä»¤
            crawler_dir = os.path.join(settings.BASE_DIR, '..', 'crawler')
            output_path = os.path.join(crawler_dir, 'output', output_file)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            cmd = [
                'scrapy', 'crawl', 'avmoo_actresses_complete',
                '-a', f'max_pages={max_pages}',
                '-a', f'max_actresses={max_actresses}',
                '-s', f'DOWNLOAD_DELAY={delay}',
                '-s', f'CONCURRENT_REQUESTS={concurrent}',
                '-o', output_path,
                '-L', 'INFO' if verbose else 'WARNING'
            ]

            self.stdout.write(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            self.stdout.write(f"ğŸ“ å·¥ä½œç›®å½•: {crawler_dir}")
            self.stdout.write(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_path}")

            # æ‰§è¡Œçˆ¬è™«
            start_time = time.time()
            
            process = subprocess.Popen(
                cmd,
                cwd=crawler_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )

            # å®æ—¶è¾“å‡ºæ—¥å¿—
            for line in iter(process.stdout.readline, ''):
                if verbose:
                    self.stdout.write(line.rstrip())
                elif any(keyword in line for keyword in ['ERROR', 'WARNING', 'INFO']):
                    self.stdout.write(line.rstrip())

            process.wait()
            end_time = time.time()
            duration = end_time - start_time

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            if process.returncode == 0:
                task.status = 'completed'
                task.end_time = datetime.now()
                task.duration = duration
                task.save()

                self.stdout.write(
                    self.style.SUCCESS(f'âœ… çˆ¬è™«æ‰§è¡Œå®Œæˆ! è€—æ—¶: {duration:.2f}ç§’')
                )
                
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
                if os.path.exists(output_path):
                    file_size = os.path.getsize(output_path)
                    self.stdout.write(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                    
                    # ç®€å•ç»Ÿè®¡
                    try:
                        import json
                        with open(output_path, 'r', encoding='utf-8') as f:
                            data = [json.loads(line) for line in f if line.strip()]
                        
                        actresses = [item for item in data if item.get('data_type') == 'actress']
                        movies = [item for item in data if item.get('data_type') == 'movie']
                        
                        self.stdout.write(f"ğŸ‘© çˆ¬å–å¥³å‹æ•°é‡: {len(actresses)}")
                        self.stdout.write(f"ğŸ¬ çˆ¬å–ä½œå“æ•°é‡: {len(movies)}")
                        
                        task.result = {
                            'actresses_count': len(actresses),
                            'movies_count': len(movies),
                            'total_items': len(data)
                        }
                        task.save()
                        
                    except Exception as e:
                        self.stdout.write(
                            self.style.WARNING(f'âš ï¸ ç»Ÿè®¡æ•°æ®æ—¶å‡ºé”™: {e}')
                        )
                else:
                    self.stdout.write(
                        self.style.WARNING('âš ï¸ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨')
                    )

            else:
                task.status = 'failed'
                task.end_time = datetime.now()
                task.duration = duration
                task.error_message = f'çˆ¬è™«è¿›ç¨‹é€€å‡ºç : {process.returncode}'
                task.save()

                raise CommandError(f'âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥! é€€å‡ºç : {process.returncode}')

        except KeyboardInterrupt:
            task.status = 'cancelled'
            task.end_time = datetime.now()
            task.save()
            
            self.stdout.write(
                self.style.WARNING('â¹ï¸ çˆ¬è™«è¢«ç”¨æˆ·ä¸­æ–­')
            )
            
        except Exception as e:
            task.status = 'failed'
            task.end_time = datetime.now()
            task.error_message = str(e)
            task.save()
            
            raise CommandError(f'âŒ çˆ¬è™«æ‰§è¡Œå‡ºé”™: {e}')

        finally:
            self.stdout.write(f"ğŸ“Š ä»»åŠ¡ID: {task.id}")
            self.stdout.write(f"ğŸ“ˆ ä»»åŠ¡çŠ¶æ€: {task.status}")


class ActressesCompleteSpiderRunner:
    """å¥³å‹çˆ¬è™«è¿è¡Œå™¨"""
    
    def __init__(self, max_pages=5, max_actresses=50, delay=3, concurrent=2):
        self.max_pages = max_pages
        self.max_actresses = max_actresses
        self.delay = delay
        self.concurrent = concurrent
    
    def run(self, output_file='actresses_complete.json'):
        """è¿è¡Œçˆ¬è™«"""
        from django.core.management import call_command
        
        call_command(
            'crawl_actresses_complete',
            max_pages=self.max_pages,
            max_actresses=self.max_actresses,
            output=output_file,
            delay=self.delay,
            concurrent=self.concurrent,
            verbose=True
        )
    
    def run_async(self, output_file='actresses_complete.json'):
        """å¼‚æ­¥è¿è¡Œçˆ¬è™«"""
        from celery import current_app
        
        # å¦‚æœæœ‰Celeryï¼Œä½¿ç”¨å¼‚æ­¥ä»»åŠ¡
        if hasattr(current_app, 'send_task'):
            return current_app.send_task(
                'apps.crawler.tasks.run_actresses_complete_spider',
                args=[self.max_pages, self.max_actresses, output_file, self.delay, self.concurrent]
            )
        else:
            # å¦åˆ™åŒæ­¥è¿è¡Œ
            return self.run(output_file)


def run_actresses_complete_spider(max_pages=5, max_actresses=50, output_file='actresses_complete.json'):
    """ä¾¿æ·å‡½æ•° - è¿è¡Œå®Œæ•´å¥³å‹çˆ¬è™«"""
    runner = ActressesCompleteSpiderRunner(max_pages, max_actresses)
    return runner.run(output_file)
