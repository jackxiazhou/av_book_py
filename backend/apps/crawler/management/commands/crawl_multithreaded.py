"""
Djangoç®¡ç†å‘½ä»¤ - å¤šçº¿ç¨‹çˆ¬å–å¥³å‹ä½œå“å’Œå›¾ç‰‡
"""

import threading
import queue
import time
import random
import requests
from bs4 import BeautifulSoup
import re
import os
from urllib.parse import urljoin, urlparse
from django.core.management.base import BaseCommand
from apps.actresses.models import Actress
from apps.movies.models import Movie
from django.db import transaction
from django.conf import settings
from django.utils import timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging


class Command(BaseCommand):
    help = 'å¤šçº¿ç¨‹çˆ¬å–å¥³å‹ä½œå“å’Œå›¾ç‰‡'

    def add_arguments(self, parser):
        parser.add_argument(
            '--max-actresses',
            type=int,
            default=100,
            help='æœ€å¤§çˆ¬å–å¥³å‹æ•°é‡'
        )
        parser.add_argument(
            '--max-workers',
            type=int,
            default=5,
            help='æœ€å¤§çº¿ç¨‹æ•°'
        )
        parser.add_argument(
            '--max-movies-per-actress',
            type=int,
            default=15,
            help='æ¯ä¸ªå¥³å‹æœ€å¤§ä½œå“æ•°'
        )
        parser.add_argument(
            '--delay',
            type=float,
            default=1.0,
            help='è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰'
        )

    def handle(self, *args, **options):
        max_actresses = options['max_actresses']
        max_workers = options['max_workers']
        max_movies = options['max_movies_per_actress']
        delay = options['delay']

        self.stdout.write(
            self.style.SUCCESS(f'ğŸš€ å¯åŠ¨å¤šçº¿ç¨‹çˆ¬å– ({max_workers} çº¿ç¨‹)')
        )

        # åˆå§‹åŒ–
        self.delay = delay
        self.max_movies = max_movies
        self.session_pool = self.create_session_pool(max_workers)
        self.ensure_media_directories()

        # è·å–å¥³å‹åˆ—è¡¨
        actresses = self.get_actresses_to_crawl(max_actresses)
        self.stdout.write(f'ğŸ“‹ æ‰¾åˆ° {len(actresses)} ä¸ªå¥³å‹éœ€è¦çˆ¬å–')

        # å¤šçº¿ç¨‹çˆ¬å–
        start_time = timezone.now()
        results = self.crawl_actresses_multithreaded(actresses, max_workers)
        end_time = timezone.now()

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r['success'])
        total_movies = sum(r['movies'] for r in results)
        total_images = sum(r['images'] for r in results)
        
        self.stdout.write(f'\nğŸ‰ å¤šçº¿ç¨‹çˆ¬å–å®Œæˆ!')
        self.stdout.write(f'ğŸ“Š ç»Ÿè®¡ç»“æœ:')
        self.stdout.write(f'  æˆåŠŸå¥³å‹: {success_count}/{len(actresses)}')
        self.stdout.write(f'  æ€»ä½œå“æ•°: {total_movies}')
        self.stdout.write(f'  æ€»å›¾ç‰‡æ•°: {total_images}')
        self.stdout.write(f'  æ€»è€—æ—¶: {end_time - start_time}')
        self.stdout.write(f'  å¹³å‡é€Ÿåº¦: {len(actresses) / (end_time - start_time).total_seconds():.2f} å¥³å‹/ç§’')

    def create_session_pool(self, pool_size):
        """åˆ›å»ºä¼šè¯æ± """
        sessions = []
        for _ in range(pool_size):
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7',
            })
            sessions.append(session)
        return sessions

    def get_actresses_to_crawl(self, max_count):
        """è·å–è¦çˆ¬å–çš„å¥³å‹åˆ—è¡¨"""
        # ä¼˜å…ˆé€‰æ‹©æœ‰å¤´åƒä½†ä½œå“è¾ƒå°‘çš„å¥³å‹
        actresses = Actress.objects.exclude(
            profile_image__isnull=True
        ).exclude(
            profile_image=''
        ).order_by('?')[:max_count]  # éšæœºæ’åºé¿å…é‡å¤çˆ¬å–åŒä¸€æ‰¹
        
        return list(actresses)

    def crawl_actresses_multithreaded(self, actresses, max_workers):
        """å¤šçº¿ç¨‹çˆ¬å–å¥³å‹"""
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_actress = {
                executor.submit(self.crawl_single_actress, actress, i % len(self.session_pool)): actress 
                for i, actress in enumerate(actresses)
            }
            
            # å¤„ç†ç»“æœ
            for future in as_completed(future_to_actress):
                actress = future_to_actress[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    if result['success']:
                        self.stdout.write(f'âœ… {actress.name}: {result["movies"]} ä½œå“, {result["images"]} å›¾ç‰‡')
                    else:
                        self.stdout.write(f'âŒ {actress.name}: {result["error"]}')
                        
                except Exception as e:
                    self.stdout.write(f'âŒ {actress.name}: çº¿ç¨‹å¼‚å¸¸ {e}')
                    results.append({
                        'actress': actress.name,
                        'success': False,
                        'movies': 0,
                        'images': 0,
                        'error': str(e)
                    })
        
        return results

    def crawl_single_actress(self, actress, session_index):
        """çˆ¬å–å•ä¸ªå¥³å‹"""
        session = self.session_pool[session_index]
        result = {
            'actress': actress.name,
            'success': False,
            'movies': 0,
            'images': 0,
            'error': None
        }
        
        try:
            # æ„é€ å¥³å‹URL
            actress_url = self.get_actress_url(actress)
            if not actress_url:
                result['error'] = 'æ— æ³•æ„é€ å¥³å‹URL'
                return result
            
            # çˆ¬å–å¥³å‹é¡µé¢
            response = session.get(actress_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è·å–ä½œå“é“¾æ¥
            movie_links = soup.select('a[href*="/movie/"]')
            movie_urls = []
            for link in movie_links:
                href = link.get('href')
                if href:
                    movie_url = urljoin(actress_url, href)
                    movie_urls.append(movie_url)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_movie_urls = list(set(movie_urls))[:self.max_movies]
            
            # çˆ¬å–ä½œå“
            movies_count = 0
            images_count = 0
            
            for movie_url in unique_movie_urls:
                try:
                    movie_data = self.crawl_movie_with_images(movie_url, session)
                    if movie_data:
                        movie = self.save_movie(movie_data, actress)
                        if movie:
                            movies_count += 1
                            images_count += movie_data.get('images_downloaded', 0)
                    
                    # å»¶è¿Ÿ
                    time.sleep(self.delay + random.uniform(0, 0.5))
                    
                except Exception as e:
                    continue  # è·³è¿‡å¤±è´¥çš„ä½œå“
            
            result['success'] = True
            result['movies'] = movies_count
            result['images'] = images_count
            
            # æ›´æ–°å¥³å‹ä¿¡æ¯
            with transaction.atomic():
                actress.last_crawled_at = timezone.now()
                actress.crawl_count += 1
                actress.save()
            
        except Exception as e:
            result['error'] = str(e)
        
        return result

    def crawl_movie_with_images(self, movie_url, session):
        """çˆ¬å–ä½œå“ä¿¡æ¯å’Œå›¾ç‰‡"""
        try:
            response = session.get(movie_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            movie_data = {
                'source_url': movie_url,
                'images_downloaded': 0
            }
            
            # æå–ç•ªå·
            title_text = soup.title.text if soup.title else ''
            if title_text:
                code_match = re.search(r'([A-Z]+-\d+|[A-Z]+\d+)', title_text)
                if code_match:
                    movie_data['censored_id'] = code_match.group(1)
            
            # æå–æ ‡é¢˜
            clean_title = re.sub(r'([A-Z]+-\d+|[A-Z]+\d+)', '', title_text)
            clean_title = re.sub(r' - AVMOO.*$', '', clean_title).strip()
            if clean_title:
                movie_data['movie_title'] = clean_title
            
            # æå–å°é¢å›¾ç‰‡
            cover_img = soup.select_one('.screencap img, .bigImage img, .cover img')
            if cover_img:
                cover_src = cover_img.get('src')
                if cover_src:
                    cover_url = urljoin(movie_url, cover_src)
                    movie_data['cover_image'] = cover_url
                    
                    # ä¸‹è½½å°é¢
                    local_path = self.download_image(
                        cover_url, 
                        'cover', 
                        movie_data.get('censored_id', 'unknown'),
                        session
                    )
                    if local_path:
                        movie_data['cover_image_local'] = local_path
                        movie_data['images_downloaded'] += 1
            
            # æå–æ ·å“å›¾ç‰‡
            sample_imgs = soup.select('.sample-box img, .samples img, .preview img')
            sample_urls = []
            sample_local_paths = []
            
            for i, img in enumerate(sample_imgs[:6]):
                src = img.get('src')
                if src:
                    sample_url = urljoin(movie_url, src)
                    sample_urls.append(sample_url)
                    
                    # ä¸‹è½½æ ·å“å›¾
                    local_path = self.download_image(
                        sample_url, 
                        'sample', 
                        movie_data.get('censored_id', 'unknown'),
                        session,
                        f'sample_{i+1:02d}'
                    )
                    if local_path:
                        sample_local_paths.append(local_path)
                        movie_data['images_downloaded'] += 1
            
            if sample_urls:
                movie_data['sample_images'] = '\n'.join(sample_urls)
            
            if sample_local_paths:
                movie_data['sample_images_local'] = '\n'.join(sample_local_paths)
            
            # æå–å…¶ä»–ä¿¡æ¯
            page_text = soup.get_text()
            
            # å‘è¡Œæ—¥æœŸ
            date_match = re.search(r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})', page_text)
            if date_match:
                movie_data['release_date'] = date_match.group(1).replace('/', '-')
            
            # æ—¶é•¿
            duration_match = re.search(r'(\d+)\s*åˆ†', page_text)
            if duration_match:
                movie_data['duration_minutes'] = int(duration_match.group(1))
            
            # åˆ¶ä½œå•†
            studio_match = re.search(r'åˆ¶ä½œå•†[ï¼š:]\s*([^\n\r]+)', page_text)
            if studio_match:
                studio = studio_match.group(1).strip()
                if len(studio) < 50:
                    movie_data['studio'] = studio
            
            return movie_data
            
        except Exception as e:
            return None

    def download_image(self, image_url, image_type, movie_id, session, filename=None):
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            response = session.get(image_url, timeout=30)
            response.raise_for_status()
            
            # è·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(image_url)
            ext = os.path.splitext(parsed_url.path)[1]
            if not ext:
                ext = '.jpg'
            
            # ç”Ÿæˆæ–‡ä»¶å
            if not filename:
                filename = f'{image_type}{ext}'
            else:
                filename = f'{filename}{ext}'
            
            # åˆ›å»ºç›®å½•è·¯å¾„
            movie_dir = os.path.join(
                settings.MEDIA_ROOT, 
                'images', 
                'movies', 
                movie_id
            )
            os.makedirs(movie_dir, exist_ok=True)
            
            # ä¿å­˜æ–‡ä»¶
            file_path = os.path.join(movie_dir, filename)
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            # éªŒè¯æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            if file_size < 1024:
                os.remove(file_path)
                return None
            
            # è¿”å›ç›¸å¯¹è·¯å¾„
            relative_path = os.path.join('images', 'movies', movie_id, filename)
            return relative_path
            
        except Exception as e:
            return None

    def save_movie(self, data, actress):
        """ä¿å­˜ä½œå“ä¿¡æ¯"""
        try:
            with transaction.atomic():
                movie, created = Movie.objects.get_or_create(
                    censored_id=data.get('censored_id', ''),
                    defaults={
                        'movie_title': data.get('movie_title'),
                        'release_date': data.get('release_date'),
                        'duration_minutes': data.get('duration_minutes'),
                        'studio': data.get('studio'),
                        'cover_image': data.get('cover_image'),
                        'cover_image_local': data.get('cover_image_local'),
                        'sample_images': data.get('sample_images'),
                        'sample_images_local': data.get('sample_images_local'),
                        'source': 'multithreaded_crawl',
                    }
                )
                
                if not created:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    for field, value in data.items():
                        if value and hasattr(movie, field):
                            setattr(movie, field, value)
                    movie.save()
                
                # å»ºç«‹å¥³å‹å’Œä½œå“çš„å…³è”
                movie.actresses.add(actress)
                
                return movie
                
        except Exception as e:
            return None

    def get_actress_url(self, actress):
        """è·å–å¥³å‹URL"""
        if actress.source_url:
            return actress.source_url
        return None

    def ensure_media_directories(self):
        """ç¡®ä¿åª’ä½“ç›®å½•å­˜åœ¨"""
        media_dirs = [
            os.path.join(settings.MEDIA_ROOT, 'images'),
            os.path.join(settings.MEDIA_ROOT, 'images', 'movies'),
        ]
        
        for dir_path in media_dirs:
            os.makedirs(dir_path, exist_ok=True)
