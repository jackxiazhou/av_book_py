"""
Djangoç®¡ç†å‘½ä»¤ - æ·±åº¦é€’å½’çˆ¬å–
ä»ä½œå“é¡µé¢ç»§ç»­é€’å½’å…¶ä»–å‚æ¼”å¥³å‹ï¼Œå®ç°å¤šå±‚é€’å½’çˆ¬å–ç½‘ç»œ
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import random
from urllib.parse import urljoin
from django.core.management.base import BaseCommand
from apps.actresses.models import Actress
from apps.movies.models import Movie
from django.db import transaction
from django.utils import timezone
import json
from collections import deque


class Command(BaseCommand):
    help = 'æ·±åº¦é€’å½’çˆ¬å–å¥³å‹ç½‘ç»œ'

    def add_arguments(self, parser):
        parser.add_argument(
            '--start-actress-url',
            type=str,
            help='èµ·å§‹å¥³å‹URL'
        )
        parser.add_argument(
            '--start-actress-id',
            type=str,
            help='èµ·å§‹å¥³å‹ID'
        )
        parser.add_argument(
            '--max-depth',
            type=int,
            default=3,
            help='æœ€å¤§é€’å½’æ·±åº¦ (é»˜è®¤: 3)'
        )
        parser.add_argument(
            '--max-actresses-per-level',
            type=int,
            default=5,
            help='æ¯å±‚æœ€å¤§å¥³å‹æ•° (é»˜è®¤: 5)'
        )
        parser.add_argument(
            '--max-movies-per-actress',
            type=int,
            default=10,
            help='æ¯ä¸ªå¥³å‹æœ€å¤§ä½œå“æ•° (é»˜è®¤: 10)'
        )
        parser.add_argument(
            '--delay',
            type=int,
            default=5,
            help='è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰'
        )
        parser.add_argument(
            '--save-network',
            action='store_true',
            help='ä¿å­˜ç½‘ç»œå…³ç³»åˆ°æ–‡ä»¶'
        )

    def handle(self, *args, **options):
        start_url = options.get('start_actress_url')
        start_id = options.get('start_actress_id')
        max_depth = options['max_depth']
        max_actresses_per_level = options['max_actresses_per_level']
        max_movies = options['max_movies_per_actress']
        delay = options['delay']
        save_network = options['save_network']

        if not start_url and not start_id:
            self.stdout.write(
                self.style.ERROR('è¯·æä¾› --start-actress-url æˆ– --start-actress-id å‚æ•°')
            )
            return

        if start_id and not start_url:
            start_url = f'https://avmoo.website/cn/star/{start_id}'

        self.stdout.write(
            self.style.SUCCESS(f'ğŸ•¸ï¸ å¼€å§‹æ·±åº¦é€’å½’çˆ¬å– (æœ€å¤§æ·±åº¦: {max_depth})')
        )
        self.stdout.write(f'èµ·å§‹å¥³å‹: {start_url}')

        # åˆå§‹åŒ–
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7',
        })

        # é€’å½’çˆ¬å–çŠ¶æ€
        self.crawled_actresses = set()  # å·²çˆ¬å–çš„å¥³å‹URL
        self.crawled_movies = set()     # å·²çˆ¬å–çš„ä½œå“URL
        self.actress_network = {}       # å¥³å‹ç½‘ç»œå…³ç³»
        self.delay = delay

        try:
            # å¼€å§‹æ·±åº¦é€’å½’
            network = self.deep_crawl(
                start_url, 
                max_depth, 
                max_actresses_per_level, 
                max_movies
            )

            self.stdout.write(
                self.style.SUCCESS(f'ğŸ‰ æ·±åº¦é€’å½’å®Œæˆ!')
            )
            
            # æ˜¾ç¤ºç»Ÿè®¡
            self.show_network_stats(network)

            # ä¿å­˜ç½‘ç»œå…³ç³»
            if save_network:
                self.save_network_to_file(network)

        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f'âŒ æ·±åº¦é€’å½’å‡ºé”™: {e}')
            )

    def deep_crawl(self, start_url, max_depth, max_actresses_per_level, max_movies):
        """æ·±åº¦é€’å½’çˆ¬å–"""
        # ä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢
        queue = deque([(start_url, 0)])  # (url, depth)
        network = {
            'actresses': {},
            'movies': {},
            'relationships': []
        }

        while queue:
            current_url, depth = queue.popleft()
            
            if depth >= max_depth:
                continue
                
            if current_url in self.crawled_actresses:
                continue

            self.stdout.write(f'\nğŸ­ [æ·±åº¦ {depth}] çˆ¬å–å¥³å‹: {current_url}')
            
            # çˆ¬å–å½“å‰å¥³å‹
            actress_data = self.crawl_actress_with_movies(current_url, max_movies)
            if not actress_data:
                continue

            self.crawled_actresses.add(current_url)
            
            # ä¿å­˜å¥³å‹ä¿¡æ¯
            actress_id = actress_data['actress_id']
            network['actresses'][actress_id] = actress_data
            
            self.stdout.write(f'  âœ… å¥³å‹: {actress_data.get("name", "Unknown")}')
            self.stdout.write(f'  ğŸ¬ ä½œå“: {len(actress_data.get("movies", []))} ä¸ª')

            # ä»ä½œå“ä¸­å‘ç°æ–°å¥³å‹
            new_actresses = []
            for movie_data in actress_data.get('movies', []):
                movie_id = movie_data.get('censored_id')
                if movie_id:
                    network['movies'][movie_id] = movie_data
                    
                    # è®°å½•å…³ç³»
                    network['relationships'].append({
                        'actress_id': actress_id,
                        'movie_id': movie_id,
                        'type': 'stars_in'
                    })

                # è·å–ä½œå“ä¸­çš„å…¶ä»–å¥³å‹
                co_actresses = movie_data.get('co_actresses', [])
                for co_actress in co_actresses:
                    if co_actress['url'] not in self.crawled_actresses:
                        new_actresses.append(co_actress['url'])

            # é™åˆ¶æ¯å±‚çš„å¥³å‹æ•°é‡
            if new_actresses:
                selected_actresses = random.sample(
                    new_actresses, 
                    min(len(new_actresses), max_actresses_per_level)
                )
                
                self.stdout.write(f'  ğŸ”— å‘ç° {len(new_actresses)} ä¸ªå…³è”å¥³å‹ï¼Œé€‰æ‹© {len(selected_actresses)} ä¸ª')
                
                # æ·»åŠ åˆ°é˜Ÿåˆ—
                for actress_url in selected_actresses:
                    queue.append((actress_url, depth + 1))

            # å»¶è¿Ÿ
            time.sleep(self.delay + random.uniform(0, 2))

        return network

    def crawl_actress_with_movies(self, actress_url, max_movies):
        """çˆ¬å–å¥³å‹åŠå…¶ä½œå“ä¿¡æ¯"""
        try:
            # çˆ¬å–å¥³å‹åŸºæœ¬ä¿¡æ¯
            response = self.session.get(actress_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            actress_data = {
                'url': actress_url,
                'movies': []
            }
            
            # æå–å¥³å‹åŸºæœ¬ä¿¡æ¯
            actress_id_match = re.search(r'/star/([a-f0-9]+)', actress_url)
            if actress_id_match:
                actress_data['actress_id'] = actress_id_match.group(1)
            
            # æå–å§“å
            title_text = soup.title.text if soup.title else ''
            if title_text and ' - ' in title_text:
                actress_data['name'] = title_text.split(' - ')[0].strip()
            
            # è·å–ä½œå“é“¾æ¥
            movie_links = soup.select('a[href*="/movie/"]')
            movie_urls = []
            for link in movie_links:
                href = link.get('href')
                if href:
                    movie_url = urljoin(actress_url, href)
                    movie_urls.append(movie_url)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_movie_urls = list(set(movie_urls))[:max_movies]
            
            # çˆ¬å–æ¯ä¸ªä½œå“çš„è¯¦æƒ…
            for i, movie_url in enumerate(unique_movie_urls):
                if movie_url in self.crawled_movies:
                    continue
                    
                self.stdout.write(f'    ğŸ¬ [{i+1}/{len(unique_movie_urls)}] çˆ¬å–ä½œå“')
                
                movie_data = self.crawl_movie_with_actresses(movie_url)
                if movie_data:
                    actress_data['movies'].append(movie_data)
                    self.crawled_movies.add(movie_url)
                
                # ä½œå“é—´å»¶è¿Ÿ
                if i < len(unique_movie_urls) - 1:
                    time.sleep(1 + random.uniform(0, 1))
            
            return actress_data
            
        except Exception as e:
            self.stdout.write(f'    âŒ çˆ¬å–å¥³å‹å¤±è´¥: {e}')
            return None

    def crawl_movie_with_actresses(self, movie_url):
        """çˆ¬å–ä½œå“åŠå‚æ¼”å¥³å‹ä¿¡æ¯"""
        try:
            response = self.session.get(movie_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            movie_data = {
                'url': movie_url,
                'co_actresses': []
            }
            
            # æå–ç•ªå·
            title_text = soup.title.text if soup.title else ''
            if title_text:
                code_match = re.search(r'([A-Z]+-\d+|[A-Z]+\d+)', title_text)
                if code_match:
                    movie_data['censored_id'] = code_match.group(1)
            
            # æå–æ ‡é¢˜
            clean_title = re.sub(r'([A-Z]+-\d+|[A-Z]+\d+)', '', title_text)
            clean_title = re.sub(r' - AVMOO.*$', '', clean_title).strip()
            if clean_title:
                movie_data['title'] = clean_title
            
            # è·å–å‚æ¼”å¥³å‹
            actress_links = soup.select('a[href*="/star/"]')
            for link in actress_links:
                href = link.get('href')
                name = link.get_text().strip()
                if href and name:
                    actress_url = urljoin(movie_url, href)
                    movie_data['co_actresses'].append({
                        'name': name,
                        'url': actress_url
                    })
            
            return movie_data
            
        except Exception as e:
            self.stdout.write(f'      âŒ çˆ¬å–ä½œå“å¤±è´¥: {e}')
            return None

    def show_network_stats(self, network):
        """æ˜¾ç¤ºç½‘ç»œç»Ÿè®¡"""
        actresses_count = len(network['actresses'])
        movies_count = len(network['movies'])
        relationships_count = len(network['relationships'])
        
        self.stdout.write(f'\nğŸ“Š ç½‘ç»œç»Ÿè®¡:')
        self.stdout.write(f'  å¥³å‹æ•°é‡: {actresses_count}')
        self.stdout.write(f'  ä½œå“æ•°é‡: {movies_count}')
        self.stdout.write(f'  å…³ç³»æ•°é‡: {relationships_count}')
        
        # è®¡ç®—ç½‘ç»œå¯†åº¦
        if actresses_count > 1:
            max_relationships = actresses_count * movies_count
            density = relationships_count / max_relationships * 100
            self.stdout.write(f'  ç½‘ç»œå¯†åº¦: {density:.2f}%')
        
        # æ˜¾ç¤ºçƒ­é—¨å¥³å‹ï¼ˆå‚æ¼”ä½œå“æœ€å¤šï¼‰
        actress_movie_counts = {}
        for rel in network['relationships']:
            actress_id = rel['actress_id']
            actress_movie_counts[actress_id] = actress_movie_counts.get(actress_id, 0) + 1
        
        if actress_movie_counts:
            top_actresses = sorted(
                actress_movie_counts.items(), 
                key=lambda x: x[1], 
                reverse=True
            )[:5]
            
            self.stdout.write(f'\nğŸŒŸ çƒ­é—¨å¥³å‹ (ä½œå“æ•°):')
            for actress_id, movie_count in top_actresses:
                actress_name = network['actresses'].get(actress_id, {}).get('name', 'Unknown')
                self.stdout.write(f'  {actress_name}: {movie_count} éƒ¨ä½œå“')

    def save_network_to_file(self, network):
        """ä¿å­˜ç½‘ç»œå…³ç³»åˆ°æ–‡ä»¶"""
        try:
            filename = f'actress_network_{timezone.now().strftime("%Y%m%d_%H%M%S")}.json'
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(network, f, ensure_ascii=False, indent=2)
            
            self.stdout.write(f'ğŸ“ ç½‘ç»œå…³ç³»å·²ä¿å­˜åˆ°: {filename}')
        except Exception as e:
            self.stdout.write(f'âŒ ä¿å­˜ç½‘ç»œæ–‡ä»¶å¤±è´¥: {e}')
