"""
Djangoç®¡ç†å‘½ä»¤ - è‡ªåŠ¨å‘ç°å¥³å‹åˆ—è¡¨
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import random
from urllib.parse import urljoin, urlparse
from django.core.management.base import BaseCommand
from apps.actresses.models import Actress
from django.db import transaction
import json


class Command(BaseCommand):
    help = 'è‡ªåŠ¨å‘ç°å¥³å‹åˆ—è¡¨é¡µçš„æ‰€æœ‰å¥³å‹'

    def add_arguments(self, parser):
        parser.add_argument(
            '--start-page',
            type=int,
            default=1,
            help='èµ·å§‹é¡µç  (é»˜è®¤: 1)'
        )
        parser.add_argument(
            '--max-pages',
            type=int,
            default=10,
            help='æœ€å¤§é¡µæ•° (é»˜è®¤: 10)'
        )
        parser.add_argument(
            '--delay',
            type=int,
            default=3,
            help='é¡µé¢é—´å»¶è¿Ÿï¼ˆç§’ï¼‰'
        )
        parser.add_argument(
            '--save-urls',
            action='store_true',
            help='ä¿å­˜å¥³å‹URLåˆ°æ–‡ä»¶'
        )
        parser.add_argument(
            '--output-file',
            type=str,
            default='discovered_actresses.json',
            help='è¾“å‡ºæ–‡ä»¶å'
        )

    def handle(self, *args, **options):
        start_page = options['start_page']
        max_pages = options['max_pages']
        delay = options['delay']
        save_urls = options['save_urls']
        output_file = options['output_file']

        self.stdout.write(
            self.style.SUCCESS(f'ğŸ” å¼€å§‹å‘ç°å¥³å‹åˆ—è¡¨ (é¡µç  {start_page}-{start_page + max_pages - 1})')
        )

        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7',
        })

        discovered_actresses = []
        total_found = 0

        try:
            for page in range(start_page, start_page + max_pages):
                self.stdout.write(f'ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µ...')
                
                actresses = self.discover_page(page)
                if actresses:
                    discovered_actresses.extend(actresses)
                    total_found += len(actresses)
                    self.stdout.write(f'  âœ… å‘ç° {len(actresses)} ä¸ªå¥³å‹')
                else:
                    self.stdout.write(f'  âŒ ç¬¬ {page} é¡µæ²¡æœ‰å‘ç°å¥³å‹')
                
                # å»¶è¿Ÿ
                if page < start_page + max_pages - 1:
                    delay_time = delay + random.uniform(0, 2)
                    self.stdout.write(f'  â±ï¸ ç­‰å¾… {delay_time:.1f} ç§’...')
                    time.sleep(delay_time)

            self.stdout.write(
                self.style.SUCCESS(f'ğŸ‰ å‘ç°å®Œæˆï¼æ€»å…±æ‰¾åˆ° {total_found} ä¸ªå¥³å‹')
            )

            # å»é‡
            unique_actresses = self.deduplicate_actresses(discovered_actresses)
            self.stdout.write(f'ğŸ“Š å»é‡å: {len(unique_actresses)} ä¸ªå”¯ä¸€å¥³å‹')

            # ä¿å­˜åˆ°æ–‡ä»¶
            if save_urls:
                self.save_to_file(unique_actresses, output_file)

            # ä¿å­˜åˆ°æ•°æ®åº“
            saved_count = self.save_to_database(unique_actresses)
            self.stdout.write(f'ğŸ’¾ ä¿å­˜åˆ°æ•°æ®åº“: {saved_count} ä¸ªå¥³å‹')

        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f'âŒ å‘ç°è¿‡ç¨‹å‡ºé”™: {e}')
            )

    def discover_page(self, page):
        """å‘ç°å•é¡µå¥³å‹"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„å¥³å‹åˆ—è¡¨URL
        base_urls = [
            f'https://avmoo.website/cn/star?page={page}',
            f'https://avmoo.website/star?page={page}',
            f'https://avmoo.cyou/cn/star?page={page}',
            f'https://avmoo.cyou/star?page={page}',
        ]

        for base_url in base_urls:
            try:
                response = self.session.get(base_url, timeout=30)
                if response.status_code == 200:
                    return self.parse_actress_list(response, base_url)
            except Exception as e:
                self.stdout.write(f'    å°è¯• {base_url} å¤±è´¥: {e}')
                continue

        return []

    def parse_actress_list(self, response, base_url):
        """è§£æå¥³å‹åˆ—è¡¨é¡µé¢"""
        soup = BeautifulSoup(response.content, 'html.parser')
        actresses = []

        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = [
            'a[href*="/star/"]',
            '.actress-item a',
            '.star-item a',
            '.grid-item a',
            '.thumbnail a'
        ]

        for selector in selectors:
            links = soup.select(selector)
            if links:
                self.stdout.write(f'    ä½¿ç”¨é€‰æ‹©å™¨: {selector} (æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥)')
                
                for link in links:
                    href = link.get('href')
                    if href and '/star/' in href:
                        # æå–å¥³å‹ID
                        actress_id_match = re.search(r'/star/([a-f0-9]+)', href)
                        if actress_id_match:
                            actress_id = actress_id_match.group(1)
                            actress_url = urljoin(base_url, href)
                            
                            # å°è¯•è·å–å¥³å‹å§“å
                            name = self.extract_actress_name(link)
                            
                            actress_info = {
                                'actress_id': actress_id,
                                'name': name,
                                'url': actress_url,
                                'discovered_from': base_url
                            }
                            actresses.append(actress_info)
                
                if actresses:
                    break  # æ‰¾åˆ°å¥³å‹å°±åœæ­¢å°è¯•å…¶ä»–é€‰æ‹©å™¨

        return actresses

    def extract_actress_name(self, link_element):
        """ä»é“¾æ¥å…ƒç´ ä¸­æå–å¥³å‹å§“å"""
        # å°è¯•å¤šç§æ–¹å¼è·å–å§“å
        name_sources = [
            link_element.get('title'),
            link_element.get_text().strip(),
        ]
        
        # æŸ¥æ‰¾å›¾ç‰‡çš„altå±æ€§
        img = link_element.find('img')
        if img:
            name_sources.extend([
                img.get('alt'),
                img.get('title')
            ])

        for name in name_sources:
            if name and name.strip():
                # æ¸…ç†å§“å
                clean_name = re.sub(r'\s+', ' ', name.strip())
                if len(clean_name) > 0 and len(clean_name) < 50:
                    return clean_name

        return None

    def deduplicate_actresses(self, actresses):
        """å»é‡å¥³å‹åˆ—è¡¨"""
        seen_ids = set()
        unique_actresses = []
        
        for actress in actresses:
            actress_id = actress['actress_id']
            if actress_id not in seen_ids:
                seen_ids.add(actress_id)
                unique_actresses.append(actress)
        
        return unique_actresses

    def save_to_file(self, actresses, filename):
        """ä¿å­˜å¥³å‹åˆ—è¡¨åˆ°æ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(actresses, f, ensure_ascii=False, indent=2)
            
            self.stdout.write(f'ğŸ“ å¥³å‹åˆ—è¡¨å·²ä¿å­˜åˆ°: {filename}')
        except Exception as e:
            self.stdout.write(f'âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}')

    def save_to_database(self, actresses):
        """ä¿å­˜å¥³å‹åŸºæœ¬ä¿¡æ¯åˆ°æ•°æ®åº“"""
        saved_count = 0
        
        for actress_info in actresses:
            try:
                with transaction.atomic():
                    actress, created = Actress.objects.get_or_create(
                        name=actress_info.get('name', ''),
                        defaults={
                            'source': 'discovered',
                            'source_url': actress_info.get('url'),
                        }
                    )
                    
                    if created:
                        saved_count += 1
                        self.stdout.write(f'  âœ… æ–°å¢å¥³å‹: {actress.name}')
                    else:
                        # æ›´æ–°URLå¦‚æœä¸ºç©º
                        if not actress.source_url and actress_info.get('url'):
                            actress.source_url = actress_info.get('url')
                            actress.save()
                        self.stdout.write(f'  â„¹ï¸ å·²å­˜åœ¨: {actress.name}')
                        
            except Exception as e:
                self.stdout.write(f'  âŒ ä¿å­˜å¤±è´¥ {actress_info.get("name", "Unknown")}: {e}')
        
        return saved_count

    def get_discovered_actresses_stats(self):
        """è·å–å‘ç°çš„å¥³å‹ç»Ÿè®¡"""
        total_actresses = Actress.objects.count()
        discovered_actresses = Actress.objects.filter(source='discovered').count()
        
        self.stdout.write(f'\nğŸ“Š å¥³å‹ç»Ÿè®¡:')
        self.stdout.write(f'  æ€»å¥³å‹æ•°: {total_actresses}')
        self.stdout.write(f'  å·²å‘ç°å¥³å‹: {discovered_actresses}')
        self.stdout.write(f'  å¾…çˆ¬å–å¥³å‹: {discovered_actresses}')

        return {
            'total': total_actresses,
            'discovered': discovered_actresses,
        }
