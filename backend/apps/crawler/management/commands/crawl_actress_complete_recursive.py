"""
Djangoç®¡ç†å‘½ä»¤ - é€’å½’çˆ¬å–å¥³å‹å®Œæ•´ä¿¡æ¯
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import random
from urllib.parse import urljoin
from django.core.management.base import BaseCommand
from apps.actresses.models import Actress
from apps.movies.models import Movie
from django.db import transaction
import json


class Command(BaseCommand):
    help = 'é€’å½’çˆ¬å–å¥³å‹å®Œæ•´ä¿¡æ¯å’Œç›¸å…³ä½œå“'

    def add_arguments(self, parser):
        parser.add_argument(
            '--actress-url',
            type=str,
            help='å¥³å‹è¯¦æƒ…é¡µURL'
        )
        parser.add_argument(
            '--actress-id',
            type=str,
            help='å¥³å‹ID'
        )
        parser.add_argument(
            '--max-movies',
            type=int,
            default=20,
            help='æœ€å¤§çˆ¬å–ä½œå“æ•°é‡'
        )
        parser.add_argument(
            '--delay',
            type=int,
            default=3,
            help='è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰'
        )

    def handle(self, *args, **options):
        actress_url = options.get('actress_url')
        actress_id = options.get('actress_id')
        max_movies = options['max_movies']
        delay = options['delay']

        if not actress_url and not actress_id:
            self.stdout.write(
                self.style.ERROR('è¯·æä¾› --actress-url æˆ– --actress-id å‚æ•°')
            )
            return

        if actress_id and not actress_url:
            actress_url = f'https://avmoo.website/cn/star/{actress_id}'

        self.stdout.write(
            self.style.SUCCESS(f'ğŸ•·ï¸ å¼€å§‹é€’å½’çˆ¬å–å¥³å‹ä¿¡æ¯: {actress_url}')
        )

        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7',
        })

        try:
            # çˆ¬å–å¥³å‹ä¿¡æ¯
            actress_data = self.crawl_actress(actress_url)
            if not actress_data:
                self.stdout.write(self.style.ERROR('âŒ å¥³å‹ä¿¡æ¯çˆ¬å–å¤±è´¥'))
                return

            # ä¿å­˜å¥³å‹ä¿¡æ¯
            actress = self.save_actress(actress_data)
            if not actress:
                self.stdout.write(self.style.ERROR('âŒ å¥³å‹ä¿¡æ¯ä¿å­˜å¤±è´¥'))
                return

            self.stdout.write(f'âœ… å¥³å‹ä¿¡æ¯ä¿å­˜æˆåŠŸ: {actress.name}')

            # çˆ¬å–ä½œå“ä¿¡æ¯
            movie_urls = actress_data.get('movie_urls', [])[:max_movies]
            self.stdout.write(f'ğŸ¬ å¼€å§‹çˆ¬å– {len(movie_urls)} ä¸ªä½œå“')

            movies_saved = 0
            for i, movie_url in enumerate(movie_urls, 1):
                self.stdout.write(f'  çˆ¬å–ä½œå“ {i}/{len(movie_urls)}: {movie_url}')
                
                movie_data = self.crawl_movie(movie_url)
                if movie_data:
                    movie = self.save_movie(movie_data, actress)
                    if movie:
                        movies_saved += 1
                        self.stdout.write(f'    âœ… ä¿å­˜æˆåŠŸ: {movie.censored_id}')
                    else:
                        self.stdout.write(f'    âŒ ä¿å­˜å¤±è´¥')
                else:
                    self.stdout.write(f'    âŒ çˆ¬å–å¤±è´¥')

                # å»¶è¿Ÿ
                if i < len(movie_urls):
                    time.sleep(delay + random.uniform(0, 2))

            self.stdout.write(
                self.style.SUCCESS(f'ğŸ‰ çˆ¬å–å®Œæˆï¼ä¿å­˜äº† {movies_saved} ä¸ªä½œå“')
            )

        except Exception as e:
            self.stdout.write(
                self.style.ERROR(f'âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}')
            )

    def crawl_actress(self, url):
        """çˆ¬å–å¥³å‹ä¿¡æ¯"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            actress_data = {
                'source_url': url,
                'movie_urls': []
            }
            
            # æå–å¥³å‹ID
            actress_id_match = re.search(r'/star/([a-f0-9]+)', url)
            if actress_id_match:
                actress_data['actress_id'] = actress_id_match.group(1)
            
            # æå–å§“å
            title_text = soup.title.text if soup.title else ''
            if title_text and ' - ' in title_text:
                actress_data['name'] = title_text.split(' - ')[0].strip()
            
            # æå–å¤´åƒ
            img_elements = soup.select('img')
            for img in img_elements:
                src = img.get('src')
                if src and any(keyword in src.lower() for keyword in ['avatar', 'photo', 'image']) and src.endswith(('.jpg', '.png', '.jpeg')):
                    actress_data['profile_image'] = urljoin(url, src)
                    break
            
            # æå–è¯¦ç»†ä¿¡æ¯
            page_text = soup.get_text()
            
            # ç”Ÿæ—¥
            birthday_patterns = [
                r'ç”Ÿæ—¥[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'Birthday[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'å‡ºç”Ÿ[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})'
            ]
            for pattern in birthday_patterns:
                match = re.search(pattern, page_text)
                if match:
                    actress_data['birth_date'] = match.group(1).replace('/', '-')
                    break
            
            # èº«é«˜
            height_patterns = [
                r'èº«é«˜[ï¼š:]\s*(\d+)\s*cm',
                r'Height[ï¼š:]\s*(\d+)\s*cm'
            ]
            for pattern in height_patterns:
                match = re.search(pattern, page_text)
                if match:
                    height = int(match.group(1))
                    if 140 <= height <= 200:
                        actress_data['height'] = height
                        break
            
            # ç½©æ¯
            cup_patterns = [
                r'ç½©æ¯[ï¼š:]\s*([A-Z]+)',
                r'Cup[ï¼š:]\s*([A-Z]+)'
            ]
            for pattern in cup_patterns:
                match = re.search(pattern, page_text)
                if match:
                    actress_data['cup_size'] = match.group(1)
                    break
            
            # ä¸‰å›´
            measurements_patterns = [
                r'ä¸‰å›´[ï¼š:]\s*(\d+[-/]\d+[-/]\d+)',
                r'BWH[ï¼š:]\s*(\d+[-/]\d+[-/]\d+)'
            ]
            for pattern in measurements_patterns:
                match = re.search(pattern, page_text)
                if match:
                    actress_data['measurements'] = match.group(1).replace('/', '-')
                    break
            
            # è·å–ä½œå“é“¾æ¥
            movie_links = soup.select('a[href*="/movie/"]')
            for link in movie_links:
                href = link.get('href')
                if href:
                    movie_url = urljoin(url, href)
                    actress_data['movie_urls'].append(movie_url)
            
            # å»é‡
            actress_data['movie_urls'] = list(set(actress_data['movie_urls']))
            
            return actress_data
            
        except Exception as e:
            self.stdout.write(f'çˆ¬å–å¥³å‹ä¿¡æ¯å¤±è´¥: {e}')
            return None

    def crawl_movie(self, url):
        """çˆ¬å–ä½œå“ä¿¡æ¯"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            movie_data = {
                'source_url': url
            }
            
            # æå–ç•ªå·å’Œæ ‡é¢˜
            title_text = soup.title.text if soup.title else ''
            if title_text:
                # ä»æ ‡é¢˜ä¸­æå–ç•ªå·
                code_match = re.search(r'([A-Z]+-\d+|[A-Z]+\d+)', title_text)
                if code_match:
                    movie_data['censored_id'] = code_match.group(1)
                
                # æå–æ ‡é¢˜ï¼ˆå»æ‰ç•ªå·å’Œç½‘ç«™åï¼‰
                clean_title = re.sub(r'([A-Z]+-\d+|[A-Z]+\d+)', '', title_text)
                clean_title = re.sub(r' - AVMOO.*$', '', clean_title).strip()
                if clean_title:
                    movie_data['movie_title'] = clean_title
            
            # æå–å°é¢å›¾ç‰‡
            img_elements = soup.select('img')
            for img in img_elements:
                src = img.get('src')
                if src and src.endswith(('.jpg', '.png', '.jpeg')):
                    movie_data['cover_image'] = urljoin(url, src)
                    break
            
            # æå–é¡µé¢æ–‡æœ¬ä¿¡æ¯
            page_text = soup.get_text()
            
            # å‘è¡Œæ—¥æœŸ
            date_patterns = [
                r'å‘è¡Œæ—¥æœŸ[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'Release Date[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})'
            ]
            for pattern in date_patterns:
                match = re.search(pattern, page_text)
                if match:
                    movie_data['release_date'] = match.group(1).replace('/', '-')
                    break
            
            # æ—¶é•¿
            duration_patterns = [
                r'æ—¶é•¿[ï¼š:]\s*(\d+)\s*åˆ†',
                r'Duration[ï¼š:]\s*(\d+)\s*min',
                r'(\d+)\s*åˆ†é’Ÿ'
            ]
            for pattern in duration_patterns:
                match = re.search(pattern, page_text)
                if match:
                    movie_data['duration_minutes'] = int(match.group(1))
                    break
            
            # åˆ¶ä½œå•†
            studio_patterns = [
                r'åˆ¶ä½œå•†[ï¼š:]\s*([^\n\r]+)',
                r'Studio[ï¼š:]\s*([^\n\r]+)'
            ]
            for pattern in studio_patterns:
                match = re.search(pattern, page_text)
                if match:
                    studio = match.group(1).strip()
                    if len(studio) < 50:  # é¿å…è¿‡é•¿çš„æ–‡æœ¬
                        movie_data['studio'] = studio
                        break
            
            return movie_data
            
        except Exception as e:
            self.stdout.write(f'çˆ¬å–ä½œå“ä¿¡æ¯å¤±è´¥: {e}')
            return None

    def save_actress(self, data):
        """ä¿å­˜å¥³å‹ä¿¡æ¯"""
        try:
            with transaction.atomic():
                actress, created = Actress.objects.get_or_create(
                    name=data.get('name', ''),
                    defaults={
                        'birth_date': data.get('birth_date'),
                        'height': data.get('height'),
                        'cup_size': data.get('cup_size'),
                        'measurements': data.get('measurements'),
                        'profile_image': data.get('profile_image'),
                        'source': 'avmoo_recursive',
                    }
                )
                
                if not created:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    for field, value in data.items():
                        if value and hasattr(actress, field):
                            setattr(actress, field, value)
                    actress.save()
                
                return actress
                
        except Exception as e:
            self.stdout.write(f'ä¿å­˜å¥³å‹ä¿¡æ¯å¤±è´¥: {e}')
            return None

    def save_movie(self, data, actress):
        """ä¿å­˜ä½œå“ä¿¡æ¯"""
        try:
            with transaction.atomic():
                movie, created = Movie.objects.get_or_create(
                    censored_id=data.get('censored_id', ''),
                    defaults={
                        'movie_title': data.get('movie_title'),
                        'release_date': data.get('release_date'),
                        'duration_minutes': data.get('duration_minutes'),
                        'studio': data.get('studio'),
                        'cover_image': data.get('cover_image'),
                        'source': 'avmoo_recursive',
                    }
                )
                
                if not created:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    for field, value in data.items():
                        if value and hasattr(movie, field):
                            setattr(movie, field, value)
                    movie.save()
                
                # å»ºç«‹å¥³å‹å’Œä½œå“çš„å…³è”
                movie.actresses.add(actress)
                
                return movie
                
        except Exception as e:
            self.stdout.write(f'ä¿å­˜ä½œå“ä¿¡æ¯å¤±è´¥: {e}')
            return None
