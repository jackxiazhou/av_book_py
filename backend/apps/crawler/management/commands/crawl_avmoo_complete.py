"""
å®Œæ•´çš„AVMooçˆ¬è™«ç³»ç»Ÿ
1. çˆ¬å–å¥³å‹åˆ—è¡¨é¡µ https://avmoo.website/cn/actresses
2. çˆ¬å–å¥³å‹è¯¦æƒ…é¡µ https://avmoo.website/cn/star/xxx
3. çˆ¬å–å¥³å‹çš„æ‰€æœ‰ä½œå“å¹¶å»ºç«‹å…³è”
4. çˆ¬å–ä½œå“è¯¦æƒ…é¡µå’Œæ ·ä¾‹å›¾ç‰‡
"""

import requests
from bs4 import BeautifulSoup
import time
import re
from urllib.parse import urljoin, urlparse
from django.core.management.base import BaseCommand
from django.utils import timezone
from apps.movies.models import Movie, MovieRating
from apps.actresses.models import Actress, ActressTag
from apps.crawler.models import CrawlerSession, CrawlerLog
from apps.crawler.utils.image_downloader import ImageDownloader
import random
from datetime import datetime


class AVMooCompleteCrawler:
    def __init__(self, proxy_url=None, download_images=True):
        self.session = requests.Session()
        self.proxy_url = proxy_url
        self.download_images = download_images
        
        if proxy_url:
            self.session.proxies = {
                'http': proxy_url,
                'https': proxy_url
            }
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://avmoo.website/',
        })
        
        # åˆå§‹åŒ–å›¾ç‰‡ä¸‹è½½å™¨
        if self.download_images:
            self.image_downloader = ImageDownloader(proxy_url=proxy_url)
        
        self.base_url = 'https://avmoo.website'
        self.actresses_processed = 0
        self.movies_processed = 0
        self.request_count = 0

        # å¾…çˆ¬å–å¥³å‹é˜Ÿåˆ—
        self.pending_actresses = set()
        self.processed_actress_urls = set()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'actresses_created': 0,
            'actresses_updated': 0,
            'movies_created': 0,
            'movies_updated': 0,
            'relationships_created': 0,
            'images_downloaded': 0
        }
    
    def get_page(self, url, timeout=30, max_retries=3):
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    delay = random.uniform(3, 8)
                    print(f"Retry {attempt}, waiting {delay:.1f}s...")
                    time.sleep(delay)
                
                print(f"Requesting: {url}")
                response = self.session.get(url, timeout=timeout)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                self.request_count += 1
                
                # æ·»åŠ è¯·æ±‚é—´éš”
                time.sleep(random.uniform(2, 5))
                
                return response
                
            except Exception as e:
                print(f"Request failed (attempt {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    return None
        
        return None
    
    def crawl_actresses_list(self, max_pages=10):
        """çˆ¬å–å¥³å‹åˆ—è¡¨é¡µé¢"""
        print("=== å¼€å§‹çˆ¬å–å¥³å‹åˆ—è¡¨ ===")
        
        actress_urls = []
        
        for page in range(1, max_pages + 1):
            if page == 1:
                list_url = f"{self.base_url}/cn/actresses"
            else:
                list_url = f"{self.base_url}/cn/actresses?page={page}"
            
            print(f"çˆ¬å–å¥³å‹åˆ—è¡¨é¡µé¢ {page}: {list_url}")
            response = self.get_page(list_url)
            
            if not response:
                print(f"æ— æ³•è·å–é¡µé¢ {page}")
                continue
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾å¥³å‹é“¾æ¥
            page_actress_urls = self.extract_actress_urls(soup, list_url)
            
            if not page_actress_urls:
                print(f"é¡µé¢ {page} æ²¡æœ‰æ‰¾åˆ°å¥³å‹é“¾æ¥ï¼Œå¯èƒ½å·²åˆ°æœ€åä¸€é¡µ")
                break
            
            actress_urls.extend(page_actress_urls)
            print(f"é¡µé¢ {page} æ‰¾åˆ° {len(page_actress_urls)} ä¸ªå¥³å‹é“¾æ¥")
        
        print(f"æ€»å…±æ‰¾åˆ° {len(actress_urls)} ä¸ªå¥³å‹é“¾æ¥")
        return actress_urls
    
    def extract_actress_urls(self, soup, base_url):
        """ä»é¡µé¢ä¸­æå–å¥³å‹é“¾æ¥"""
        actress_urls = []
        
        # å¤šç§é€‰æ‹©å™¨å°è¯•
        selectors = [
            'a[href*="/star/"]',
            '.actress-box a',
            '.star-box a',
            '.avatar a',
            '.photo-frame a'
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            if links:
                for link in links:
                    href = link.get('href')
                    if href and '/star/' in href:
                        full_url = urljoin(base_url, href)
                        actress_urls.append(full_url)
                break
        
        return list(set(actress_urls))  # å»é‡
    
    def crawl_actress_detail(self, actress_url):
        """çˆ¬å–å¥³å‹è¯¦æƒ…é¡µé¢"""
        print(f"çˆ¬å–å¥³å‹è¯¦æƒ…: {actress_url}")

        response = self.get_page(actress_url)
        if not response:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        # æå–å¥³å‹åŸºæœ¬ä¿¡æ¯
        actress_data = self.extract_actress_info(soup, actress_url)

        if not actress_data.get('name'):
            print(f"æ— æ³•æå–å¥³å‹å§“å: {actress_url}")
            return None

        # ä¿å­˜å¥³å‹ä¿¡æ¯
        actress = self.save_actress(actress_data)

        if actress:
            # æ£€æŸ¥æ˜¯å¦å·²ç»çˆ¬å–è¿‡ä½œå“
            if actress.movies_crawled:
                print(f"å¥³å‹ {actress.name} å·²çˆ¬å–è¿‡ä½œå“ï¼Œè·³è¿‡")
                return actress

            # çˆ¬å–å¥³å‹çš„æ‰€æœ‰ä½œå“
            movie_urls = self.crawl_actress_movies(actress_url, actress)
            print(f"å¥³å‹ {actress.name} æ‰¾åˆ° {len(movie_urls)} éƒ¨ä½œå“")

            # æ ‡è®°ä¸ºå·²çˆ¬å–
            actress.movies_crawled = True
            actress.crawl_date = timezone.now()
            actress.save()
            print(f"å¥³å‹ {actress.name} ä½œå“çˆ¬å–å®Œæˆ")

        return actress
    
    def extract_actress_info(self, soup, url):
        """æå–å¥³å‹ä¿¡æ¯"""
        data = {}
        
        # å§“åæå–
        name_selectors = [
            '.avatar-box .photo-info span',
            '.star-name',
            'h1',
            '.title'
        ]
        
        for selector in name_selectors:
            elem = soup.select_one(selector)
            if elem:
                name = elem.get_text().strip()
                if name and len(name) > 1:
                    data['name'] = name
                    break
        
        # ä»URLæå–å§“åä½œä¸ºå¤‡é€‰
        if not data.get('name'):
            url_match = re.search(r'/star/([^/]+)', url)
            if url_match:
                data['name'] = url_match.group(1).replace('-', ' ')
        
        # æå–ä¸ªäººä¿¡æ¯
        text_content = soup.get_text()
        
        # ç”Ÿæ—¥
        birthday_match = re.search(r'ç”Ÿæ—¥[ï¼š:]\s*(\d{4}-\d{2}-\d{2})', text_content)
        if birthday_match:
            try:
                data['birth_date'] = datetime.strptime(birthday_match.group(1), '%Y-%m-%d').date()
            except ValueError:
                pass
        
        # èº«é«˜
        height_match = re.search(r'èº«é«˜[ï¼š:]\s*(\d+)', text_content)
        if height_match:
            data['height'] = int(height_match.group(1))
        
        # ä½“é‡
        weight_match = re.search(r'ä½“é‡[ï¼š:]\s*(\d+)', text_content)
        if weight_match:
            data['weight'] = int(weight_match.group(1))
        
        # ä¸‰å›´
        measurements_match = re.search(r'ä¸‰å›´[ï¼š:]\s*([^\\n]+)', text_content)
        if measurements_match:
            data['measurements'] = measurements_match.group(1).strip()
        
        # ç½©æ¯
        cup_match = re.search(r'ç½©æ¯[ï¼š:]\s*([A-Z]+)', text_content)
        if cup_match:
            data['cup_size'] = cup_match.group(1)
        
        # è¡€å‹
        blood_match = re.search(r'è¡€å‹[ï¼š:]\s*([ABO]+)', text_content)
        if blood_match:
            data['blood_type'] = blood_match.group(1)
        
        # å‡ºé“æ—¥æœŸ
        debut_match = re.search(r'å‡ºé“[ï¼š:]\s*(\d{4}-\d{2}-\d{2})', text_content)
        if debut_match:
            try:
                data['debut_date'] = datetime.strptime(debut_match.group(1), '%Y-%m-%d').date()
            except ValueError:
                pass
        
        # æå–å›¾ç‰‡
        if self.download_images:
            data.update(self.extract_actress_images(soup, url, data.get('name', 'unknown')))
        
        # é»˜è®¤å€¼
        data['nationality'] = 'æ—¥æœ¬'
        data['is_active'] = True
        data['source_url'] = url
        data['movies_crawled'] = False  # æ ‡è®°æ˜¯å¦å·²çˆ¬å–è¿‡ä½œå“

        return data
    
    def extract_actress_images(self, soup, url, actress_name):
        """æå–å¥³å‹å›¾ç‰‡"""
        data = {}
        
        # å¤´åƒ
        profile_selectors = [
            '.avatar-box .photo-frame img',
            '.profile-image img',
            '.star-photo img'
        ]
        
        for selector in profile_selectors:
            img = soup.select_one(selector)
            if img:
                img_url = img.get('src') or img.get('data-src')
                if img_url:
                    img_url = urljoin(url, img_url)
                    local_path = self.image_downloader.download_image(
                        img_url, 
                        'actress_profile',
                        f"{actress_name}_profile"
                    )
                    if local_path:
                        data['profile_image'] = self.image_downloader.get_image_url(local_path)
                        self.stats['images_downloaded'] += 1
                    break
        
        # å°é¢å›¾ç‰‡
        cover_selectors = [
            '.cover-image img',
            '.banner img',
            '.header-image img'
        ]
        
        for selector in cover_selectors:
            img = soup.select_one(selector)
            if img:
                img_url = img.get('src') or img.get('data-src')
                if img_url:
                    img_url = urljoin(url, img_url)
                    local_path = self.image_downloader.download_image(
                        img_url,
                        'actress_cover', 
                        f"{actress_name}_cover"
                    )
                    if local_path:
                        data['cover_image'] = self.image_downloader.get_image_url(local_path)
                        self.stats['images_downloaded'] += 1
                    break
        
        return data
    
    def crawl_actress_movies(self, actress_url, actress):
        """çˆ¬å–å¥³å‹çš„æ‰€æœ‰ä½œå“"""
        movie_urls = []
        page = 1
        
        while True:
            if page == 1:
                page_url = actress_url
            else:
                page_url = f"{actress_url}?page={page}"
            
            print(f"çˆ¬å–å¥³å‹ä½œå“é¡µé¢ {page}: {page_url}")
            response = self.get_page(page_url)
            
            if not response:
                break
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–ä½œå“é“¾æ¥
            page_movie_urls = self.extract_movie_urls(soup, page_url)
            
            if not page_movie_urls:
                print(f"å¥³å‹ä½œå“é¡µé¢ {page} æ²¡æœ‰æ‰¾åˆ°ä½œå“ï¼Œç»“æŸçˆ¬å–")
                break
            
            movie_urls.extend(page_movie_urls)
            print(f"å¥³å‹ä½œå“é¡µé¢ {page} æ‰¾åˆ° {len(page_movie_urls)} éƒ¨ä½œå“")
            
            # å¤„ç†æ¯éƒ¨ä½œå“
            for movie_url in page_movie_urls:
                self.crawl_movie_detail(movie_url, actress)
            
            page += 1
            
            # é™åˆ¶æœ€å¤§é¡µæ•°
            if page > 20:
                break
        
        return movie_urls
    
    def extract_movie_urls(self, soup, base_url):
        """ä»é¡µé¢ä¸­æå–ä½œå“é“¾æ¥"""
        movie_urls = []
        
        selectors = [
            'a[href*="/movie/"]',
            '.movie-box a',
            '.item a',
            '.video-box a'
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            if links:
                for link in links:
                    href = link.get('href')
                    if href and '/movie/' in href:
                        full_url = urljoin(base_url, href)
                        movie_urls.append(full_url)
                break
        
        return list(set(movie_urls))  # å»é‡

    def crawl_movie_detail(self, movie_url, primary_actress=None):
        """çˆ¬å–ä½œå“è¯¦æƒ…é¡µé¢"""
        print(f"çˆ¬å–ä½œå“è¯¦æƒ…: {movie_url}")

        response = self.get_page(movie_url)
        if not response:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        # æå–ä½œå“ä¿¡æ¯
        movie_data = self.extract_movie_info(soup, movie_url)

        if not movie_data.get('censored_id'):
            print(f"æ— æ³•æå–ä½œå“ç¼–å·: {movie_url}")
            return None

        # ä¿å­˜ä½œå“ä¿¡æ¯
        movie = self.save_movie(movie_data)

        if movie:
            # å¤„ç†ä½œå“ä¸­çš„æ‰€æœ‰å¥³å‹
            actresses_in_movie = self.process_movie_actresses(soup, movie_url, movie)

            # å¦‚æœæœ‰ä¸»è¦å¥³å‹ï¼Œç¡®ä¿å»ºç«‹å…³è”
            if primary_actress:
                if not movie.actresses.filter(id=primary_actress.id).exists():
                    movie.actresses.add(primary_actress)
                    self.stats['relationships_created'] += 1
                    print(f"å»ºç«‹ä¸»è¦å…³è”: {primary_actress.name} <-> {movie.censored_id}")

            print(f"ä½œå“ {movie.censored_id} å…³è”äº† {len(actresses_in_movie)} ä½å¥³å‹")

        return movie

    def process_movie_actresses(self, soup, movie_url, movie):
        """å¤„ç†ä½œå“ä¸­çš„æ‰€æœ‰å¥³å‹"""
        actresses_processed = []

        # æå–ä½œå“ä¸­çš„æ‰€æœ‰æ¼”å‘˜
        idol_elems = soup.select('p:contains("æ¼”å“¡") a, .star a, a[href*="/star/"]')

        for idol_elem in idol_elems:
            actress_name = idol_elem.get_text().strip()
            actress_url = idol_elem.get('href')

            if not actress_name or not actress_url:
                continue

            # è½¬æ¢ä¸ºç»å¯¹URL
            actress_url = urljoin(movie_url, actress_url)

            print(f"  å¤„ç†ä½œå“ä¸­çš„å¥³å‹: {actress_name}")

            # æ£€æŸ¥å¥³å‹æ˜¯å¦å·²å­˜åœ¨
            actress = Actress.objects.filter(name=actress_name).first()

            if not actress:
                # åˆ›å»ºæ–°å¥³å‹ï¼ˆåŸºæœ¬ä¿¡æ¯ï¼‰
                actress_data = {
                    'name': actress_name,
                    'nationality': 'æ—¥æœ¬',
                    'is_active': True,
                    'source_url': actress_url,
                    'movies_crawled': False,
                    'description': f'ä»ä½œå“ {movie.censored_id} ä¸­å‘ç°çš„å¥³å‹'
                }
                actress = self.save_actress(actress_data)
                print(f"    åˆ›å»ºæ–°å¥³å‹: {actress_name}")

            if actress:
                # å»ºç«‹å…³è”
                if not movie.actresses.filter(id=actress.id).exists():
                    movie.actresses.add(actress)
                    self.stats['relationships_created'] += 1
                    print(f"    å»ºç«‹å…³è”: {actress_name} <-> {movie.censored_id}")

                actresses_processed.append(actress)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–å¥³å‹è¯¦æƒ…
                if not actress.movies_crawled and actress.source_url:
                    print(f"    å¥³å‹ {actress_name} æœªçˆ¬å–è¿‡ï¼ŒåŠ å…¥å¾…çˆ¬å–é˜Ÿåˆ—")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­ï¼Œç¨åå¤„ç†
                    self.pending_actresses.add(actress_url)

        return actresses_processed

    def extract_movie_info(self, soup, url):
        """æå–ä½œå“ä¿¡æ¯"""
        data = {}

        # æå–ä½œå“ç¼–å·
        censored_id = self.extract_censored_id(soup, url)
        if not censored_id:
            return data

        data['censored_id'] = censored_id
        data['source'] = 'avmoo_complete'
        data['source_url'] = url

        # æ ‡é¢˜
        title_selectors = ['h3', '.title', 'title']
        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                title = elem.get_text().strip()
                if title and censored_id not in title:
                    data['movie_title'] = title
                    break

        # å°é¢å›¾ç‰‡
        cover_selectors = ['.bigImage img', '.cover img', '.poster img']
        for selector in cover_selectors:
            img = soup.select_one(selector)
            if img:
                img_url = img.get('src') or img.get('data-src')
                if img_url:
                    data['movie_pic_cover'] = urljoin(url, img_url)
                    break

        # å‘è¡Œæ—¥æœŸ
        text_content = soup.get_text()
        date_match = re.search(r'ç™¼è¡Œæ—¥æœŸ[ï¼š:]\s*(\d{4}-\d{2}-\d{2})', text_content)
        if date_match:
            try:
                data['release_date'] = datetime.strptime(date_match.group(1), '%Y-%m-%d').date()
            except ValueError:
                pass

        # æ—¶é•¿
        length_match = re.search(r'é•·åº¦[ï¼š:]\s*([^<\\n]+)', text_content)
        if length_match:
            data['movie_length'] = length_match.group(1).strip()

        # å¯¼æ¼”
        director_elem = soup.select_one('p:contains("å°æ¼”") a, .director a')
        if director_elem:
            data['director'] = director_elem.get_text().strip()

        # åˆ¶ä½œå•†
        studio_elem = soup.select_one('p:contains("è£½ä½œå•†") a, .studio a')
        if studio_elem:
            data['studio'] = studio_elem.get_text().strip()

        # å‘è¡Œå•†
        label_elem = soup.select_one('p:contains("ç™¼è¡Œå•†") a, .label a')
        if label_elem:
            data['label'] = label_elem.get_text().strip()

        # ç³»åˆ—
        series_elem = soup.select_one('p:contains("ç³»åˆ—") a, .series a')
        if series_elem:
            data['series'] = series_elem.get_text().strip()

        # ç±»åˆ«
        genre_elems = soup.select('p:contains("é¡åˆ¥") a, .genre a')
        if genre_elems:
            genres = [elem.get_text().strip() for elem in genre_elems]
            data['genre'] = ', '.join(genres)

        # æ¼”å‘˜
        idol_elems = soup.select('p:contains("æ¼”å“¡") a, .star a, a[href*="/star/"]')
        if idol_elems:
            idols = [elem.get_text().strip() for elem in idol_elems if elem.get_text().strip()]
            data['jav_idols'] = ', '.join(idols)

        # æ ·ä¾‹å›¾ç‰‡
        if self.download_images:
            data['sample_images'] = self.extract_sample_images(soup, url, censored_id)

        # å½±ç‰‡æ ‡è®°
        tag_elems = soup.select('.genre a, .tag a, .label a')
        if tag_elems:
            tags = [elem.get_text().strip() for elem in tag_elems]
            data['movie_tags'] = ', '.join(list(set(tags)))

        return data

    def extract_censored_id(self, soup, url):
        """æå–ä½œå“ç¼–å·"""
        # ä»URLä¸­æå–
        url_match = re.search(r'/movie/([^/]+)', url)
        if url_match:
            return url_match.group(1).upper()

        # ä»é¡µé¢å†…å®¹ä¸­æå–
        patterns = [
            r'è­˜åˆ¥ç¢¼[ï¼š:]\s*([A-Z0-9-]+)',
            r'å“ç•ª[ï¼š:]\s*([A-Z0-9-]+)',
            r'ç•ªè™Ÿ[ï¼š:]\s*([A-Z0-9-]+)',
        ]

        text = soup.get_text()
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip().upper()

        return None

    def extract_sample_images(self, soup, url, movie_id):
        """æå–æ ·ä¾‹å›¾ç‰‡"""
        sample_urls = []

        # æŸ¥æ‰¾æ ·ä¾‹å›¾ç‰‡
        sample_selectors = [
            '.sample-box img',
            '.preview-images img',
            '.sample-waterfall img',
            'a[href*="sample"] img'
        ]

        for selector in sample_selectors:
            imgs = soup.select(selector)
            for img in imgs:
                src = img.get('src') or img.get('data-src')
                if src:
                    full_url = urljoin(url, src)

                    # ä¸‹è½½æ ·ä¾‹å›¾ç‰‡
                    local_path = self.image_downloader.download_image(
                        full_url,
                        'movie_sample',
                        f"{movie_id}_sample_{len(sample_urls)+1}"
                    )
                    if local_path:
                        sample_urls.append(self.image_downloader.get_image_url(local_path))
                        self.stats['images_downloaded'] += 1
                    else:
                        sample_urls.append(full_url)  # ä¿ç•™åŸURLä½œä¸ºå¤‡é€‰

        return '\\n'.join(sample_urls[:10])  # æœ€å¤š10å¼ æ ·ä¾‹å›¾ç‰‡

    def save_actress(self, actress_data):
        """ä¿å­˜å¥³å‹åˆ°æ•°æ®åº“"""
        try:
            actress, created = Actress.objects.get_or_create(
                name=actress_data['name'],
                defaults=actress_data
            )

            if created:
                self.stats['actresses_created'] += 1
                print(f"åˆ›å»ºå¥³å‹: {actress.name}")

                # æ·»åŠ æ ‡ç­¾
                self.add_actress_tags(actress)
            else:
                # æ›´æ–°ç°æœ‰å¥³å‹ä¿¡æ¯
                updated = False
                for field, value in actress_data.items():
                    if field != 'name' and value and not getattr(actress, field):
                        setattr(actress, field, value)
                        updated = True

                if updated:
                    actress.save()
                    self.stats['actresses_updated'] += 1
                    print(f"æ›´æ–°å¥³å‹: {actress.name}")

            return actress

        except Exception as e:
            print(f"ä¿å­˜å¥³å‹å¤±è´¥ {actress_data.get('name', 'unknown')}: {e}")
            return None

    def save_movie(self, movie_data):
        """ä¿å­˜ä½œå“åˆ°æ•°æ®åº“"""
        try:
            movie, created = Movie.objects.get_or_create(
                censored_id=movie_data['censored_id'],
                defaults=movie_data
            )

            if created:
                self.stats['movies_created'] += 1
                print(f"åˆ›å»ºä½œå“: {movie.censored_id}")
                # åˆ›å»ºå¯¹åº”çš„è¯„åˆ†è®°å½•
                MovieRating.objects.get_or_create(movie=movie)
            else:
                # æ›´æ–°ç°æœ‰ä½œå“ä¿¡æ¯
                updated = False
                for field, value in movie_data.items():
                    if field != 'censored_id' and value:
                        current_value = getattr(movie, field)
                        if not current_value or (field == 'sample_images' and len(value) > len(str(current_value))):
                            setattr(movie, field, value)
                            updated = True

                if updated:
                    movie.save()
                    self.stats['movies_updated'] += 1
                    print(f"æ›´æ–°ä½œå“: {movie.censored_id}")

            return movie

        except Exception as e:
            print(f"ä¿å­˜ä½œå“å¤±è´¥ {movie_data.get('censored_id', 'unknown')}: {e}")
            return None

    def add_actress_tags(self, actress):
        """ä¸ºå¥³å‹æ·»åŠ æ ‡ç­¾"""
        try:
            # AVMooæ¥æºæ ‡ç­¾
            avmoo_tag, _ = ActressTag.objects.get_or_create(
                name='AVMoo',
                defaults={'slug': 'avmoo', 'color': '#17a2b8', 'description': 'ä»AVMooçˆ¬å–çš„å¥³å‹'}
            )
            avmoo_tag.actresses.add(actress)

            # æ ¹æ®ä½œå“æ•°æ·»åŠ å…¶ä»–æ ‡ç­¾ï¼ˆç¨åæ›´æ–°ï¼‰

        except Exception as e:
            print(f"æ·»åŠ æ ‡ç­¾å¤±è´¥ {actress.name}: {e}")

    def update_actress_stats(self):
        """æ›´æ–°å¥³å‹ç»Ÿè®¡ä¿¡æ¯"""
        print("=== æ›´æ–°å¥³å‹ç»Ÿè®¡ä¿¡æ¯ ===")

        for actress in Actress.objects.all():
            # æ›´æ–°ä½œå“æ•°
            movie_count = actress.movies.count()
            if actress.movie_count != movie_count:
                actress.movie_count = movie_count
                actress.popularity_score = min(movie_count * 3, 100)
                actress.save()

                # æ ¹æ®ä½œå“æ•°æ·»åŠ æ ‡ç­¾
                if movie_count > 20:
                    popular_tag, _ = ActressTag.objects.get_or_create(
                        name='äººæ°”',
                        defaults={'slug': 'popular', 'color': '#ffd700', 'description': 'äººæ°”å¥³å‹'}
                    )
                    popular_tag.actresses.add(actress)

                if movie_count > 10:
                    active_tag, _ = ActressTag.objects.get_or_create(
                        name='æ´»è·ƒ',
                        defaults={'slug': 'active', 'color': '#28a745', 'description': 'æ´»è·ƒå¥³å‹'}
                    )
                    active_tag.actresses.add(actress)

        print("å¥³å‹ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å®Œæˆ")

    def process_pending_actresses(self, max_pending=50):
        """å¤„ç†å¾…çˆ¬å–çš„å¥³å‹é˜Ÿåˆ—"""
        print(f"=== å¤„ç†å¾…çˆ¬å–å¥³å‹é˜Ÿåˆ— ({len(self.pending_actresses)} ä¸ª) ===")

        processed_count = 0
        for actress_url in list(self.pending_actresses):
            if processed_count >= max_pending:
                break

            if actress_url in self.processed_actress_urls:
                continue

            print(f"å¤„ç†å¾…çˆ¬å–å¥³å‹: {actress_url}")

            # çˆ¬å–å¥³å‹è¯¦æƒ…
            actress = self.crawl_actress_detail(actress_url)

            if actress:
                processed_count += 1
                self.processed_actress_urls.add(actress_url)
                print(f"å®Œæˆå¾…çˆ¬å–å¥³å‹: {actress.name}")

            # ä»é˜Ÿåˆ—ä¸­ç§»é™¤
            self.pending_actresses.discard(actress_url)

        print(f"å¤„ç†äº† {processed_count} ä½å¾…çˆ¬å–å¥³å‹")


class Command(BaseCommand):
    help = 'Complete AVMoo crawler: actresses list -> actress details -> movies -> movie details'

    def add_arguments(self, parser):
        parser.add_argument('--max-actresses', type=int, default=20, help='Maximum actresses to crawl')
        parser.add_argument('--max-pages', type=int, default=5, help='Maximum actress list pages to crawl')
        parser.add_argument('--proxy', type=str, help='Proxy server URL')
        parser.add_argument('--delay', type=int, default=5, help='Download delay in seconds')
        parser.add_argument('--no-images', action='store_true', help='Skip image downloading')
        parser.add_argument('--session-id', type=str, help='Custom session ID')
        parser.add_argument('--actresses-only', action='store_true', help='Only crawl actresses, skip movies')

    def handle(self, *args, **options):
        max_actresses = options['max_actresses']
        max_pages = options['max_pages']
        proxy = options.get('proxy')
        delay = options['delay']
        download_images = not options['no_images']
        custom_session_id = options.get('session_id')
        actresses_only = options['actresses_only']

        session_id = custom_session_id or f"avmoo_complete_{int(time.time())}"
        session = CrawlerSession.objects.create(
            session_id=session_id,
            crawler_type='avmoo_complete',
            total_pages=max_pages,
            max_movies=max_actresses,
            delay_seconds=delay,
            proxy_url=proxy or ''
        )

        self.stdout.write(self.style.SUCCESS('=== AVMooå®Œæ•´çˆ¬è™«å¼€å§‹ ==='))
        self.stdout.write(f'ä¼šè¯ID: {session_id}')
        self.stdout.write(f'æœ€å¤§å¥³å‹æ•°: {max_actresses}')
        self.stdout.write(f'æœ€å¤§é¡µæ•°: {max_pages}')
        self.stdout.write(f'ä¸‹è½½å›¾ç‰‡: {download_images}')
        self.stdout.write(f'ä»…çˆ¬å¥³å‹: {actresses_only}')

        # æ˜¾ç¤ºåˆå§‹ç»Ÿè®¡
        self.show_initial_stats()

        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = AVMooCompleteCrawler(proxy_url=proxy, download_images=download_images)

        try:
            # ç¬¬ä¸€æ­¥ï¼šçˆ¬å–å¥³å‹åˆ—è¡¨
            self.stdout.write(self.style.WARNING('\\n=== ç¬¬ä¸€æ­¥ï¼šçˆ¬å–å¥³å‹åˆ—è¡¨ ==='))
            actress_urls = crawler.crawl_actresses_list(max_pages)

            if not actress_urls:
                self.stdout.write(self.style.ERROR('æœªæ‰¾åˆ°å¥³å‹é“¾æ¥'))
                session.mark_failed('No actress URLs found')
                return

            # é™åˆ¶å¥³å‹æ•°é‡
            actress_urls = actress_urls[:max_actresses]
            self.stdout.write(f'å°†çˆ¬å– {len(actress_urls)} ä½å¥³å‹')

            # ç¬¬äºŒæ­¥ï¼šçˆ¬å–å¥³å‹è¯¦æƒ…å’Œä½œå“
            self.stdout.write(self.style.WARNING('\\n=== ç¬¬äºŒæ­¥ï¼šçˆ¬å–å¥³å‹è¯¦æƒ…å’Œä½œå“ ==='))

            for i, actress_url in enumerate(actress_urls, 1):
                self.stdout.write(f'\\nå¤„ç†å¥³å‹ {i}/{len(actress_urls)}: {actress_url}')

                try:
                    actress = crawler.crawl_actress_detail(actress_url)
                    if actress:
                        session.update_progress(processed=i, created=crawler.stats['actresses_created'])

                        if not actresses_only:
                            self.stdout.write(f'å¥³å‹ {actress.name} å¤„ç†å®Œæˆ')
                        else:
                            self.stdout.write(f'å¥³å‹ {actress.name} åŸºæœ¬ä¿¡æ¯çˆ¬å–å®Œæˆï¼ˆè·³è¿‡ä½œå“ï¼‰')

                    # æ·»åŠ å»¶è¿Ÿ
                    time.sleep(delay)

                except Exception as e:
                    self.stdout.write(f'å¤„ç†å¥³å‹å¤±è´¥: {e}')
                    continue

            # ç¬¬ä¸‰æ­¥ï¼šå¤„ç†å¾…çˆ¬å–å¥³å‹é˜Ÿåˆ—
            if crawler.pending_actresses:
                self.stdout.write(self.style.WARNING('\\n=== ç¬¬ä¸‰æ­¥ï¼šå¤„ç†å¾…çˆ¬å–å¥³å‹é˜Ÿåˆ— ==='))
                crawler.process_pending_actresses(max_pending=20)

            # ç¬¬å››æ­¥ï¼šæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stdout.write(self.style.WARNING('\\n=== ç¬¬å››æ­¥ï¼šæ›´æ–°ç»Ÿè®¡ä¿¡æ¯ ==='))
            crawler.update_actress_stats()

            session.mark_completed()

            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            self.show_final_stats(crawler.stats)

            self.stdout.write(self.style.SUCCESS('\\n=== AVMooå®Œæ•´çˆ¬è™«å®Œæˆ ==='))

        except KeyboardInterrupt:
            session.pause()
            self.stdout.write(self.style.WARNING(f'çˆ¬è™«è¢«ä¸­æ–­ï¼Œä¼šè¯ID: {session_id}'))
        except Exception as e:
            session.mark_failed(str(e))
            self.stdout.write(self.style.ERROR(f'çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}'))
            import traceback
            traceback.print_exc()

    def show_initial_stats(self):
        """æ˜¾ç¤ºåˆå§‹ç»Ÿè®¡"""
        from apps.movies.models import Movie
        from apps.actresses.models import Actress

        movie_count = Movie.objects.count()
        actress_count = Actress.objects.count()
        relationships = sum(movie.actresses.count() for movie in Movie.objects.all())

        self.stdout.write('\\n=== åˆå§‹æ•°æ®ç»Ÿè®¡ ===')
        self.stdout.write(f'ç°æœ‰å½±ç‰‡æ•°: {movie_count}')
        self.stdout.write(f'ç°æœ‰å¥³å‹æ•°: {actress_count}')
        self.stdout.write(f'ç°æœ‰å…³è”æ•°: {relationships}')

    def show_final_stats(self, stats):
        """æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡"""
        from apps.movies.models import Movie
        from apps.actresses.models import Actress

        total_movies = Movie.objects.count()
        total_actresses = Actress.objects.count()
        total_relationships = sum(movie.actresses.count() for movie in Movie.objects.all())

        # å›¾ç‰‡ç»Ÿè®¡
        actresses_with_profile = Actress.objects.exclude(profile_image='').count()
        actresses_with_cover = Actress.objects.exclude(cover_image='').count()
        movies_with_samples = Movie.objects.exclude(sample_images='').count()

        self.stdout.write('\\n' + '='*60)
        self.stdout.write('=== çˆ¬å–ç»“æœç»Ÿè®¡ ===')
        self.stdout.write('='*60)

        self.stdout.write(f'ğŸ“Š æ–°å¢å¥³å‹: {stats["actresses_created"]}')
        self.stdout.write(f'ğŸ“Š æ›´æ–°å¥³å‹: {stats["actresses_updated"]}')
        self.stdout.write(f'ğŸ“Š æ–°å¢ä½œå“: {stats["movies_created"]}')
        self.stdout.write(f'ğŸ“Š æ›´æ–°ä½œå“: {stats["movies_updated"]}')
        self.stdout.write(f'ğŸ“Š æ–°å¢å…³è”: {stats["relationships_created"]}')
        self.stdout.write(f'ğŸ“Š ä¸‹è½½å›¾ç‰‡: {stats["images_downloaded"]}')

        self.stdout.write(f'\\nğŸ“ˆ æ€»å¥³å‹æ•°: {total_actresses}')
        self.stdout.write(f'ğŸ“ˆ æ€»ä½œå“æ•°: {total_movies}')
        self.stdout.write(f'ğŸ“ˆ æ€»å…³è”æ•°: {total_relationships}')

        self.stdout.write(f'\\nğŸ–¼ï¸ æœ‰å¤´åƒå¥³å‹: {actresses_with_profile} ({actresses_with_profile/total_actresses*100:.1f}%)')
        self.stdout.write(f'ğŸ–¼ï¸ æœ‰å°é¢å¥³å‹: {actresses_with_cover} ({actresses_with_cover/total_actresses*100:.1f}%)')
        self.stdout.write(f'ğŸ–¼ï¸ æœ‰æ ·ä¾‹å›¾ç‰‡ä½œå“: {movies_with_samples} ({movies_with_samples/total_movies*100:.1f}%)')

        self.stdout.write('\\n=== è®¿é—®é“¾æ¥ ===')
        self.stdout.write('ğŸŒ å¥³å‹åˆ—è¡¨: http://localhost:8000/actresses/')
        self.stdout.write('ğŸŒ å½±ç‰‡åˆ—è¡¨: http://localhost:8000/movies/')
        self.stdout.write('ğŸŒ ç®¡ç†åå°: http://localhost:8000/admin/')

        self.stdout.write('='*60)
