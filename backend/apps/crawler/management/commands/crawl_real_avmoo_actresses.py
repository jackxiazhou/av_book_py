"""
è¿è¡ŒçœŸå®çš„AVMooå¥³å‹çˆ¬è™«ï¼ˆåŸºäºScrapyï¼‰
"""

import os
import sys
import subprocess
from django.core.management.base import BaseCommand
from django.conf import settings
from apps.actresses.models import Actress
from apps.crawler.models import CrawlerSession
import time


class Command(BaseCommand):
    help = 'Run real AVMoo actress crawler with Scrapy to get real images'
    
    def add_arguments(self, parser):
        parser.add_argument('--max-pages', type=int, default=5, help='Maximum pages to crawl')
        parser.add_argument('--max-actresses', type=int, default=20, help='Maximum actresses to crawl')
        parser.add_argument('--output-dir', type=str, default='output', help='Output directory for scrapy')
        parser.add_argument('--session-id', type=str, help='Custom session ID')
        parser.add_argument('--no-images', action='store_true', help='Skip image downloading')
    
    def handle(self, *args, **options):
        max_pages = options['max_pages']
        max_actresses = options['max_actresses']
        output_dir = options['output_dir']
        custom_session_id = options.get('session_id')
        skip_images = options['no_images']
        
        session_id = custom_session_id or f"real_avmoo_actresses_{int(time.time())}"
        
        # åˆ›å»ºçˆ¬è™«ä¼šè¯
        session = CrawlerSession.objects.create(
            session_id=session_id,
            crawler_type='real_avmoo_actresses',
            total_pages=max_pages,
            max_movies=max_actresses,
            delay_seconds=5
        )
        
        self.stdout.write(self.style.SUCCESS('=== å¼€å§‹çœŸå®AVMooå¥³å‹çˆ¬è™« ==='))
        self.stdout.write(f'ä¼šè¯ID: {session_id}')
        self.stdout.write(f'æœ€å¤§é¡µæ•°: {max_pages}')
        self.stdout.write(f'æœ€å¤§å¥³å‹æ•°: {max_actresses}')
        self.stdout.write(f'ä¸‹è½½å›¾ç‰‡: {not skip_images}')
        
        # æ˜¾ç¤ºåˆå§‹ç»Ÿè®¡
        initial_count = Actress.objects.count()
        self.stdout.write(f'å½“å‰å¥³å‹æ•°: {initial_count}')
        
        try:
            # æ„å»ºScrapyå‘½ä»¤
            scrapy_cmd = self.build_scrapy_command(
                max_pages, max_actresses, output_dir, skip_images
            )
            
            self.stdout.write(f'æ‰§è¡Œå‘½ä»¤: {" ".join(scrapy_cmd)}')
            
            # è¿è¡ŒScrapyçˆ¬è™«
            result = subprocess.run(
                scrapy_cmd,
                cwd=os.path.join(settings.BASE_DIR.parent, 'crawler'),
                capture_output=True,
                text=True,
                timeout=3600  # 1å°æ—¶è¶…æ—¶
            )
            
            if result.returncode == 0:
                session.mark_completed()
                self.stdout.write(self.style.SUCCESS('çˆ¬è™«æ‰§è¡ŒæˆåŠŸï¼'))
                
                # æ˜¾ç¤ºè¾“å‡º
                if result.stdout:
                    self.stdout.write('\\n=== Scrapyè¾“å‡º ===')
                    self.stdout.write(result.stdout[-2000:])  # æ˜¾ç¤ºæœ€å2000å­—ç¬¦
                
                # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
                self.show_final_stats(initial_count)
                
            else:
                session.mark_failed(f"Scrapy exit code: {result.returncode}")
                self.stdout.write(self.style.ERROR(f'çˆ¬è™«æ‰§è¡Œå¤±è´¥ï¼Œé€€å‡ºç : {result.returncode}'))
                
                if result.stderr:
                    self.stdout.write('\\n=== é”™è¯¯ä¿¡æ¯ ===')
                    self.stdout.write(result.stderr[-1000:])  # æ˜¾ç¤ºæœ€å1000å­—ç¬¦é”™è¯¯
                
                if result.stdout:
                    self.stdout.write('\\n=== è¾“å‡ºä¿¡æ¯ ===')
                    self.stdout.write(result.stdout[-1000:])
        
        except subprocess.TimeoutExpired:
            session.mark_failed("Timeout expired")
            self.stdout.write(self.style.ERROR('çˆ¬è™«æ‰§è¡Œè¶…æ—¶'))
        
        except FileNotFoundError:
            session.mark_failed("Scrapy not found")
            self.stdout.write(self.style.ERROR('æœªæ‰¾åˆ°Scrapyå‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²å®‰è£…Scrapy'))
            self.stdout.write('å®‰è£…å‘½ä»¤: pip install scrapy scrapy-user-agents')
        
        except Exception as e:
            session.mark_failed(str(e))
            self.stdout.write(self.style.ERROR(f'æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}'))
    
    def build_scrapy_command(self, max_pages, max_actresses, output_dir, skip_images):
        """æ„å»ºScrapyå‘½ä»¤"""
        cmd = [
            'scrapy', 'crawl', 'avmoo_actresses',
            '-a', f'max_pages={max_pages}',
            '-a', f'max_actresses={max_actresses}',
            '-s', 'LOG_LEVEL=INFO',
            '-s', 'DOWNLOAD_DELAY=5',
            '-s', 'RANDOMIZE_DOWNLOAD_DELAY=True',
            '-s', 'CONCURRENT_REQUESTS=2',
            '-s', 'AUTOTHROTTLE_ENABLED=True',
            '-s', 'AUTOTHROTTLE_TARGET_CONCURRENCY=1.0',
            '-o', f'{output_dir}/actresses_%(time)s.json'
        ]
        
        # å¦‚æœè·³è¿‡å›¾ç‰‡ä¸‹è½½ï¼Œç¦ç”¨å›¾ç‰‡Pipeline
        if skip_images:
            cmd.extend([
                '-s', 'ITEM_PIPELINES={"avbook_spider.pipelines.ValidationPipeline": 300, "avbook_spider.pipelines.DuplicatesPipeline": 400, "avbook_spider.pipelines.ActressDatabasePipeline": 700}'
            ])
        
        return cmd
    
    def show_final_stats(self, initial_count):
        """æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡"""
        final_count = Actress.objects.count()
        new_actresses = final_count - initial_count
        
        # ç»Ÿè®¡å›¾ç‰‡
        with_profile = Actress.objects.exclude(profile_image='').count()
        with_cover = Actress.objects.exclude(cover_image='').count()
        with_gallery = Actress.objects.exclude(gallery_images='').count()
        
        # ç»Ÿè®¡æœ¬åœ°å›¾ç‰‡
        local_profile = Actress.objects.filter(profile_image__startswith='/media/').count()
        local_cover = Actress.objects.filter(cover_image__startswith='/media/').count()
        
        self.stdout.write('\\n' + '='*50)
        self.stdout.write('=== çˆ¬å–ç»“æœç»Ÿè®¡ ===')
        self.stdout.write('='*50)
        
        self.stdout.write(f'ğŸ“Š æ–°å¢å¥³å‹æ•°: {new_actresses}')
        self.stdout.write(f'ğŸ“Š æ€»å¥³å‹æ•°: {final_count}')
        self.stdout.write(f'ğŸ–¼ï¸ æœ‰å¤´åƒçš„å¥³å‹: {with_profile} ({with_profile/final_count*100:.1f}%)')
        self.stdout.write(f'ğŸ–¼ï¸ æœ‰å°é¢çš„å¥³å‹: {with_cover} ({with_cover/final_count*100:.1f}%)')
        self.stdout.write(f'ğŸ–¼ï¸ æœ‰å›¾ç‰‡é›†çš„å¥³å‹: {with_gallery} ({with_gallery/final_count*100:.1f}%)')
        self.stdout.write(f'ğŸ’¾ æœ¬åœ°å¤´åƒæ•°: {local_profile}')
        self.stdout.write(f'ğŸ’¾ æœ¬åœ°å°é¢æ•°: {local_cover}')
        
        # æ˜¾ç¤ºæœ€æ–°åˆ›å»ºçš„å¥³å‹
        if new_actresses > 0:
            self.stdout.write('\\n=== æœ€æ–°åˆ›å»ºçš„å¥³å‹ ===')
            latest_actresses = Actress.objects.order_by('-created_at')[:min(new_actresses, 10)]
            for actress in latest_actresses:
                has_local_images = (
                    actress.profile_image.startswith('/media/') if actress.profile_image else False
                ) or (
                    actress.cover_image.startswith('/media/') if actress.cover_image else False
                )
                image_status = 'ğŸ“· æœ‰æœ¬åœ°å›¾ç‰‡' if has_local_images else 'ğŸ”— ä»…å¤–éƒ¨é“¾æ¥'
                self.stdout.write(f'  ğŸ‘© {actress.name} - ä½œå“æ•°: {actress.movie_count} - {image_status}')
        
        self.stdout.write('\\n=== è®¿é—®é“¾æ¥ ===')
        self.stdout.write('ğŸŒ å¥³å‹åˆ—è¡¨: http://localhost:8000/actresses/')
        self.stdout.write('ğŸŒ ç®¡ç†åå°: http://localhost:8000/admin/actresses/actress/')
        
        # æ£€æŸ¥åª’ä½“ç›®å½•
        media_root = settings.MEDIA_ROOT
        if os.path.exists(media_root):
            actress_dirs = [
                'images/actresses/profiles',
                'images/actresses/covers',
                'images/actresses/galleries'
            ]
            
            total_files = 0
            total_size = 0
            
            for dir_name in actress_dirs:
                dir_path = os.path.join(media_root, dir_name)
                if os.path.exists(dir_path):
                    files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]
                    total_files += len(files)
                    
                    for file in files:
                        file_path = os.path.join(dir_path, file)
                        total_size += os.path.getsize(file_path)
            
            if total_files > 0:
                self.stdout.write(f'\\nğŸ’¾ æœ¬åœ°å›¾ç‰‡æ–‡ä»¶: {total_files} ä¸ª')
                self.stdout.write(f'ğŸ’¾ å ç”¨ç©ºé—´: {total_size / 1024 / 1024:.2f} MB')
        
        self.stdout.write('='*50)
