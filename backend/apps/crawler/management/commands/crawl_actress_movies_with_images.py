"""
Djangoç®¡ç†å‘½ä»¤ - çˆ¬å–ç°æœ‰å¥³å‹çš„ä½œå“å¹¶ä¿å­˜å›¾ç‰‡
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import random
import os
from urllib.parse import urljoin, urlparse
from django.core.management.base import BaseCommand
from apps.actresses.models import Actress
from apps.movies.models import Movie
from django.db import transaction
from django.conf import settings
from django.utils import timezone
import hashlib


class Command(BaseCommand):
    help = 'çˆ¬å–ç°æœ‰å¥³å‹çš„ä½œå“å¹¶ä¿å­˜å›¾ç‰‡'

    def add_arguments(self, parser):
        parser.add_argument(
            '--max-actresses',
            type=int,
            default=10,
            help='æœ€å¤§å¤„ç†å¥³å‹æ•°é‡ (é»˜è®¤: 10)'
        )
        parser.add_argument(
            '--max-movies-per-actress',
            type=int,
            default=20,
            help='æ¯ä¸ªå¥³å‹æœ€å¤§ä½œå“æ•° (é»˜è®¤: 20)'
        )
        parser.add_argument(
            '--delay',
            type=int,
            default=3,
            help='è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰'
        )
        parser.add_argument(
            '--download-images',
            action='store_true',
            help='ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°'
        )
        parser.add_argument(
            '--actress-name',
            type=str,
            help='æŒ‡å®šå¥³å‹å§“å'
        )
        parser.add_argument(
            '--continue-on-error',
            action='store_true',
            help='é‡åˆ°é”™è¯¯æ—¶ç»§ç»­å¤„ç†'
        )

    def handle(self, *args, **options):
        max_actresses = options['max_actresses']
        max_movies = options['max_movies_per_actress']
        delay = options['delay']
        download_images = options['download_images']
        actress_name = options.get('actress_name')
        continue_on_error = options['continue_on_error']

        self.stdout.write(
            self.style.SUCCESS('ğŸ¬ å¼€å§‹çˆ¬å–å¥³å‹ä½œå“å’Œå›¾ç‰‡')
        )

        # åˆå§‹åŒ–
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7',
        })

        # åˆ›å»ºåª’ä½“ç›®å½•
        if download_images:
            self.ensure_media_directories()

        # è·å–è¦å¤„ç†çš„å¥³å‹
        actresses = self.get_actresses_to_process(actress_name, max_actresses)
        
        if not actresses:
            self.stdout.write(
                self.style.WARNING('âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å¥³å‹')
            )
            return

        self.stdout.write(f'ğŸ“‹ æ‰¾åˆ° {len(actresses)} ä¸ªå¥³å‹éœ€è¦å¤„ç†')

        # å¼€å§‹å¤„ç†
        success_count = 0
        error_count = 0
        total_movies = 0
        total_images = 0

        for i, actress in enumerate(actresses, 1):
            self.stdout.write(f'\nğŸ‘© [{i}/{len(actresses)}] å¤„ç†å¥³å‹: {actress.name}')
            
            try:
                movies_count, images_count = self.process_actress(
                    actress, max_movies, delay, download_images
                )
                
                success_count += 1
                total_movies += movies_count
                total_images += images_count
                
                self.stdout.write(f'  âœ… æˆåŠŸ: {movies_count} ä¸ªä½œå“, {images_count} å¼ å›¾ç‰‡')
                
                # æ›´æ–°å¥³å‹çˆ¬å–æ—¶é—´
                actress.last_crawled_at = timezone.now()
                actress.crawl_count += 1
                actress.save()

            except Exception as e:
                error_count += 1
                self.stdout.write(f'  âŒ å¤±è´¥: {e}')
                
                if not continue_on_error:
                    break

            # å¥³å‹é—´å»¶è¿Ÿ
            if i < len(actresses):
                delay_time = delay + random.uniform(0, 2)
                self.stdout.write(f'  â±ï¸ ç­‰å¾… {delay_time:.1f} ç§’...')
                time.sleep(delay_time)

        # ç»Ÿè®¡ç»“æœ
        self.stdout.write(f'\nğŸ‰ å¤„ç†å®Œæˆ!')
        self.stdout.write(f'ğŸ“Š ç»Ÿè®¡ç»“æœ:')
        self.stdout.write(f'  æˆåŠŸå¥³å‹: {success_count}')
        self.stdout.write(f'  å¤±è´¥å¥³å‹: {error_count}')
        self.stdout.write(f'  æ€»ä½œå“æ•°: {total_movies}')
        self.stdout.write(f'  æ€»å›¾ç‰‡æ•°: {total_images}')

    def get_actresses_to_process(self, actress_name, max_count):
        """è·å–è¦å¤„ç†çš„å¥³å‹åˆ—è¡¨"""
        if actress_name:
            # æŒ‡å®šå¥³å‹
            actresses = Actress.objects.filter(name__icontains=actress_name)
        else:
            # ä¼˜å…ˆå¤„ç†æœ‰å¤´åƒä½†ç¼ºå°‘ä½œå“çš„å¥³å‹
            actresses = Actress.objects.exclude(
                profile_image__isnull=True
            ).exclude(
                profile_image=''
            ).filter(
                movies__isnull=True
            ).distinct()
            
            # å¦‚æœæ²¡æœ‰ï¼Œåˆ™å¤„ç†æ‰€æœ‰æœ‰å¤´åƒçš„å¥³å‹
            if not actresses.exists():
                actresses = Actress.objects.exclude(
                    profile_image__isnull=True
                ).exclude(
                    profile_image=''
                )

        return list(actresses[:max_count])

    def process_actress(self, actress, max_movies, delay, download_images):
        """å¤„ç†å•ä¸ªå¥³å‹"""
        movies_count = 0
        images_count = 0
        
        # æ„é€ å¥³å‹URL
        actress_url = self.get_actress_url(actress)
        if not actress_url:
            self.stdout.write(f'    âŒ æ— æ³•æ„é€ å¥³å‹URL')
            return movies_count, images_count

        # çˆ¬å–å¥³å‹é¡µé¢
        try:
            response = self.session.get(actress_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è·å–ä½œå“é“¾æ¥
            movie_links = soup.select('a[href*="/movie/"]')
            movie_urls = []
            for link in movie_links:
                href = link.get('href')
                if href:
                    movie_url = urljoin(actress_url, href)
                    movie_urls.append(movie_url)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_movie_urls = list(set(movie_urls))[:max_movies]
            self.stdout.write(f'    ğŸ¬ æ‰¾åˆ° {len(unique_movie_urls)} ä¸ªä½œå“')
            
            # å¤„ç†æ¯ä¸ªä½œå“
            for j, movie_url in enumerate(unique_movie_urls, 1):
                self.stdout.write(f'      [{j}/{len(unique_movie_urls)}] å¤„ç†ä½œå“')
                
                movie_data = self.crawl_movie_with_images(movie_url, download_images)
                if movie_data:
                    movie = self.save_movie(movie_data, actress)
                    if movie:
                        movies_count += 1
                        images_count += movie_data.get('images_downloaded', 0)
                
                # ä½œå“é—´å»¶è¿Ÿ
                if j < len(unique_movie_urls):
                    time.sleep(1 + random.uniform(0, 1))
            
        except Exception as e:
            self.stdout.write(f'    âŒ çˆ¬å–å¥³å‹é¡µé¢å¤±è´¥: {e}')
        
        return movies_count, images_count

    def crawl_movie_with_images(self, movie_url, download_images):
        """çˆ¬å–ä½œå“ä¿¡æ¯å’Œå›¾ç‰‡"""
        try:
            response = self.session.get(movie_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            movie_data = {
                'source_url': movie_url,
                'images_downloaded': 0
            }
            
            # æå–ç•ªå·
            title_text = soup.title.text if soup.title else ''
            if title_text:
                code_match = re.search(r'([A-Z]+-\d+|[A-Z]+\d+)', title_text)
                if code_match:
                    movie_data['censored_id'] = code_match.group(1)
            
            # æå–æ ‡é¢˜
            clean_title = re.sub(r'([A-Z]+-\d+|[A-Z]+\d+)', '', title_text)
            clean_title = re.sub(r' - AVMOO.*$', '', clean_title).strip()
            if clean_title:
                movie_data['movie_title'] = clean_title
            
            # æå–å°é¢å›¾ç‰‡
            cover_img = soup.select_one('.screencap img, .bigImage img, .cover img')
            if cover_img:
                cover_src = cover_img.get('src')
                if cover_src:
                    cover_url = urljoin(movie_url, cover_src)
                    movie_data['cover_image'] = cover_url
                    
                    if download_images:
                        local_path = self.download_image(
                            cover_url, 
                            'cover', 
                            movie_data.get('censored_id', 'unknown')
                        )
                        if local_path:
                            movie_data['cover_image_local'] = local_path
                            movie_data['images_downloaded'] += 1
            
            # æå–æ ·å“å›¾ç‰‡
            sample_imgs = soup.select('.sample-box img, .samples img, .preview img')
            sample_urls = []
            sample_local_paths = []
            
            for i, img in enumerate(sample_imgs[:6]):  # æœ€å¤š6å¼ æ ·å“å›¾
                src = img.get('src')
                if src:
                    sample_url = urljoin(movie_url, src)
                    sample_urls.append(sample_url)
                    
                    if download_images:
                        local_path = self.download_image(
                            sample_url, 
                            'sample', 
                            movie_data.get('censored_id', 'unknown'),
                            f'sample_{i+1:02d}'
                        )
                        if local_path:
                            sample_local_paths.append(local_path)
                            movie_data['images_downloaded'] += 1
            
            if sample_urls:
                movie_data['sample_images'] = '\n'.join(sample_urls)
            
            if sample_local_paths:
                movie_data['sample_images_local'] = '\n'.join(sample_local_paths)
            
            # æå–å…¶ä»–ä¿¡æ¯
            page_text = soup.get_text()
            
            # å‘è¡Œæ—¥æœŸ
            date_patterns = [
                r'å‘è¡Œæ—¥æœŸ[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'Release Date[ï¼š:]\s*(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})'
            ]
            for pattern in date_patterns:
                match = re.search(pattern, page_text)
                if match:
                    movie_data['release_date'] = match.group(1).replace('/', '-')
                    break
            
            # æ—¶é•¿
            duration_patterns = [
                r'æ—¶é•¿[ï¼š:]\s*(\d+)\s*åˆ†',
                r'Duration[ï¼š:]\s*(\d+)\s*min',
                r'(\d+)\s*åˆ†é’Ÿ'
            ]
            for pattern in duration_patterns:
                match = re.search(pattern, page_text)
                if match:
                    movie_data['duration_minutes'] = int(match.group(1))
                    break
            
            # åˆ¶ä½œå•†
            studio_patterns = [
                r'åˆ¶ä½œå•†[ï¼š:]\s*([^\n\r]+)',
                r'Studio[ï¼š:]\s*([^\n\r]+)'
            ]
            for pattern in studio_patterns:
                match = re.search(pattern, page_text)
                if match:
                    studio = match.group(1).strip()
                    if len(studio) < 50:
                        movie_data['studio'] = studio
                        break
            
            # æ ‡ç­¾
            tags = soup.select('.genre a, .tags a')
            if tags:
                tag_list = [tag.get_text().strip() for tag in tags]
                movie_data['movie_tags'] = ', '.join(tag_list)
            
            return movie_data
            
        except Exception as e:
            self.stdout.write(f'        âŒ çˆ¬å–ä½œå“å¤±è´¥: {e}')
            return None

    def download_image(self, image_url, image_type, movie_id, filename=None):
        """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°"""
        try:
            response = self.session.get(image_url, timeout=30)
            response.raise_for_status()
            
            # è·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(image_url)
            ext = os.path.splitext(parsed_url.path)[1]
            if not ext:
                ext = '.jpg'
            
            # ç”Ÿæˆæ–‡ä»¶å
            if not filename:
                filename = f'{image_type}{ext}'
            else:
                filename = f'{filename}{ext}'
            
            # åˆ›å»ºç›®å½•è·¯å¾„
            movie_dir = os.path.join(
                settings.MEDIA_ROOT, 
                'images', 
                'movies', 
                movie_id
            )
            os.makedirs(movie_dir, exist_ok=True)
            
            # ä¿å­˜æ–‡ä»¶
            file_path = os.path.join(movie_dir, filename)
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            # è¿”å›ç›¸å¯¹è·¯å¾„
            relative_path = os.path.join('images', 'movies', movie_id, filename)
            return relative_path
            
        except Exception as e:
            self.stdout.write(f'          âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥ {image_url}: {e}')
            return None

    def save_movie(self, data, actress):
        """ä¿å­˜ä½œå“ä¿¡æ¯"""
        try:
            with transaction.atomic():
                movie, created = Movie.objects.get_or_create(
                    censored_id=data.get('censored_id', ''),
                    defaults={
                        'movie_title': data.get('movie_title'),
                        'release_date': data.get('release_date'),
                        'duration_minutes': data.get('duration_minutes'),
                        'studio': data.get('studio'),
                        'cover_image': data.get('cover_image'),
                        'cover_image_local': data.get('cover_image_local'),
                        'sample_images': data.get('sample_images'),
                        'sample_images_local': data.get('sample_images_local'),
                        'movie_tags': data.get('movie_tags'),
                        'source': 'actress_movies_crawl',
                    }
                )
                
                if not created:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    for field, value in data.items():
                        if value and hasattr(movie, field):
                            setattr(movie, field, value)
                    movie.save()
                
                # å»ºç«‹å¥³å‹å’Œä½œå“çš„å…³è”
                movie.actresses.add(actress)
                
                return movie
                
        except Exception as e:
            self.stdout.write(f'        âŒ ä¿å­˜ä½œå“å¤±è´¥: {e}')
            return None

    def get_actress_url(self, actress):
        """è·å–å¥³å‹URL"""
        if actress.source_url:
            return actress.source_url
        
        # å°è¯•ä»å§“åæœç´¢æ„é€ URLï¼ˆè¿™é‡Œéœ€è¦å®é™…çš„æœç´¢é€»è¾‘ï¼‰
        return None

    def ensure_media_directories(self):
        """ç¡®ä¿åª’ä½“ç›®å½•å­˜åœ¨"""
        media_dirs = [
            os.path.join(settings.MEDIA_ROOT, 'images'),
            os.path.join(settings.MEDIA_ROOT, 'images', 'movies'),
        ]
        
        for dir_path in media_dirs:
            os.makedirs(dir_path, exist_ok=True)
        
        self.stdout.write('ğŸ“ åª’ä½“ç›®å½•å·²å‡†å¤‡å°±ç»ª')
